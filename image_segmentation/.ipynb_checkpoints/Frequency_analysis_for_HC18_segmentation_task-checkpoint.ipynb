{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlK_72NP0FzF"
   },
   "source": [
    "### **Frequency analysis for HC18 segmentation task**\n",
    "\n",
    "**Description:** In this notebook, we provide the codes for:\n",
    "\n",
    "1.   PCA, non-uniform Fourier transform & frequency error computation functions \n",
    "2.   Opening reference data and data formatting functions\n",
    "3.   Storing and saving results functions\n",
    "5.   Loop to execute the above functions for the various optimizers and seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VIZaGvnYkIE5"
   },
   "source": [
    "**STEP 1 - Frequency analysis functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0j4VxTmsEuk0",
    "outputId": "517745d0-bd15-4e1e-829c-8842620ff966"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: nfft in /usr/local/lib/python3.7/dist-packages (0.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from nfft) (1.4.1)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from nfft) (3.6.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nfft) (1.21.6)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (57.4.0)\n",
      "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (0.7.1)\n",
      "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (8.13.0)\n",
      "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (1.11.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (21.4.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->nfft) (1.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nfft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWfODsrBkL-2"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import nfft\n",
    "\n",
    "def get_pca_coeffs(x):\n",
    "\n",
    "  '''This function extracts the first PC of dataset X.\n",
    "  X: Nsamples x Nfeatures\n",
    "\n",
    "  return numpy array, numpy array\n",
    "  (first PC of X, projection of X on PC)\n",
    "  '''\n",
    "\n",
    "  pca = PCA(n_components=1)\n",
    "  pca.fit_transform(x)\n",
    "  x = np.squeeze(pca.transform(x))\n",
    "\n",
    "  return pca, x\n",
    "\n",
    "def F_effect_nn(pca_out,y,x,x_sort,FGT):\n",
    "\n",
    "    '''This function computes the frequency error of function y = x, where:\n",
    "\n",
    "    y; Nsamples x Npixels: Unet output masks flattened \n",
    "    x; Nsamples: is the first projection of input images on first PC of input images\n",
    "    x_sort: x values sorted in increasing order\n",
    "    pca_out: is the first PC of output masks\n",
    "    FGT: non-uniform Fourier coefficients of ground truch function proj(yGT,pca_out) = x.\n",
    "\n",
    "    return: numpy array\n",
    "    (Frequency error)\n",
    "    '''\n",
    "\n",
    "    # projection on pca_out\n",
    "    y = np.squeeze(pca_out.transform(y)) \n",
    "\n",
    "    # computing the non uniform Fourier transform introduced in [1]\n",
    "    # for 20 freq indexes\n",
    "    y_sort = y[np.argsort(x)]\n",
    "    FT = nfft.nfft_adjoint(x_sort,y_sort,20)\n",
    "\n",
    "    # Computes delta F \n",
    "    F_err = np.abs(F_GT - FT) / np.abs(F_GT)\n",
    "\n",
    "    return F_err\n",
    "\n",
    "#[1]: @inproceedings{xu2019training,\n",
    "#  title={Training behavior of deep neural network in frequency domain},\n",
    "#  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang},\n",
    "#  booktitle={International Conference on Neural Information Processing},\n",
    "#  pages={264--274},\n",
    "#  year={2019},\n",
    "#  organization={Springer}\n",
    "#}\n",
    "\n",
    "\n",
    "def analize_training_nn(path, tr_output, pca_out, x, x_sort, F_GT, do_FPrinciple = True):\n",
    "\n",
    "  '''\n",
    "  This function loops over training data files to compute the frequency analysis.\n",
    "    path: the folder where to find the DNN training output masks per epoch as .npy\n",
    "    pca_out: is the first PC of train output masks\n",
    "\n",
    "    x; Nsamples: is the first projection of training input images on first PC of training input images\n",
    "    x_sort: x values sorted in increasing order\n",
    "    F_GT: non-uniform Fourier coefficients of ground truch function proj(yGT,pca_out) = x (training)\n",
    "\n",
    "    return list, list, list \n",
    "    (epochs, accuracies, freq errors)\n",
    "  '''\n",
    "\n",
    "  files = [file for file in os.listdir(path) if (('train' in file) and ('.npy' in file))]\n",
    "  epochs = [int(file.split('=')[-1].split('.')[0]) for file in files ]\n",
    "  files = np.array(files)[np.argsort(np.array(epochs))]\n",
    "\n",
    "  epoch_tr = []\n",
    "  accuracy_tr = []\n",
    "  F = []\n",
    "\n",
    "  for file in files[:100]:\n",
    "\n",
    "        print(file)\n",
    "        y = np.load(path+file)\n",
    "\n",
    "        # We compute the segmentation accuracy \n",
    "        res = np.mean(np.mean(y == tr_output,axis=1))\n",
    "        accuracy_tr.append(res)\n",
    "        \n",
    "        if do_FPrinciple:\n",
    "            # Compute the frequency error for the given datset\n",
    "            F_err = F_effect_nn(pca_out,y,x,x_sort,F_GT)[np.newaxis,:]\n",
    "            F.append(F_err)\n",
    "\n",
    "        epoch_tr.append(int(file.split('=')[-1].split('.')[0])) # keep track on epochs order\n",
    "\n",
    "  return epoch_tr, accuracy_tr, F\n",
    "\n",
    "\n",
    "def analize_val(path, v_output):\n",
    "\n",
    "  '''\n",
    "    This function loops over validation data files to compute the val accuracy.\n",
    "    path: the folder where to find the DNN val output masks per epoch as .npy\n",
    "    tr_output: val ground truth output masks to compute the accuracy\n",
    "\n",
    "    return: list, list\n",
    "    (epochs, accuracies)\n",
    "  '''\n",
    "\n",
    "  files = [file for file in os.listdir(path) if (('val' in file) and ('.npy' in file))]\n",
    "  epochs = [int(file.split('=')[-1].split('.')[0]) for file in files ]\n",
    "  files = np.array(files)[np.argsort(np.array(epochs))]\n",
    "\n",
    "  accuracy = []\n",
    "  epoch = []\n",
    "\n",
    "\n",
    "  for file in files[:100]:\n",
    "\n",
    "      print(file)\n",
    "\n",
    "      y = np.load(path+file)   \n",
    "      # Compute the accuracy\n",
    "      res = np.mean(np.mean(y == v_output,axis=1))\n",
    "\n",
    "      accuracy.append(res)\n",
    "      epoch.append(int(file.split('=')[-1].split('.')[0]))\n",
    "\n",
    "  return epoch, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWWOcK_WmMrx"
   },
   "source": [
    "**STEP 1 - Opening and format data functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uxl3e4nn01Rl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "\n",
    "\n",
    "def extract_files(path):\n",
    "  ''' Function to help load data. Performs basic pre-processing on images (normalization, masks to float)\n",
    "  path: folder where the training imags are stored\n",
    "\n",
    "  return: list, list (two lists containing the images and masks respectively)\n",
    "  '''\n",
    "\n",
    "  input = []\n",
    "  output = []\n",
    "\n",
    "  def format_mask(mask):\n",
    "    return (mask>0).astype(float)\n",
    "\n",
    "  def normalize(im):\n",
    "    im = im.astype(float)\n",
    "    im = (im - np.min(im))/(np.max(im)-np.min(im))\n",
    "    return im    \n",
    "\n",
    "  for file in os.listdir(path):\n",
    "    if not '_Annotation' in file:\n",
    "\n",
    "      imp = normalize(np.array(PIL.Image.open(path+file)))\n",
    "      imo = format_mask(np.array(PIL.Image.open(path+file.replace('.png','_Annotation.png'))))\n",
    "\n",
    "      input.append(imp)\n",
    "      output.append(imo)\n",
    "\n",
    "  return input, output\n",
    "\n",
    "\n",
    "def load_data(trpath = '/content/drive/MyDrive/OptML_Data/format_train/', \n",
    "              vpath = '/content/drive/MyDrive/OptML_Data/format_val/'):\n",
    "  \n",
    "  '''This function loads the validation and training data from their respective folders and fuse them to allow the \n",
    "  automatic change of train / val split depending on random shuffling seed. \n",
    "\n",
    "  return: numpy array, numpy array (two numpy arrays with the fused images data and masks respectively)\n",
    "  '''\n",
    "\n",
    "  train_input, train_output = extract_files(trpath)\n",
    "  val_input, val_output = extract_files(vpath)\n",
    "\n",
    "  train_input, train_output = extract_files(trpath)\n",
    "  plus_input, plus_output = extract_files(vpath)\n",
    "\n",
    "  train_input_ = list(np.concatenate([train_input, plus_input]))\n",
    "  train_output_ = list(np.concatenate([train_output, plus_output]))\n",
    "\n",
    "  return train_input_, train_output_\n",
    "\n",
    "\n",
    "def split_train_val(train_input_, SEED=0):\n",
    "\n",
    "  '''This function simply split train_input_ into 300 validation samples and 699 \n",
    "  training samples after random shuffling with seed SEED.\n",
    "\n",
    "  Return: numpy array, numpy array, numpy array \n",
    "  (train images, train masks, val images, val masks resp.) \n",
    "  '''\n",
    "\n",
    "  indexes = np.arange(len(train_input_))\n",
    "  np.random.seed(SEED)\n",
    "  np.random.shuffle(indexes)\n",
    "\n",
    "  v_indexes = indexes[:300]\n",
    "  tr_indexes = indexes[300:]\n",
    "\n",
    "  train_input = np.array(train_input_)[tr_indexes,:]\n",
    "  train_output = np.array(train_output_)[tr_indexes,:]\n",
    "\n",
    "  val_input = np.array(train_input_)[v_indexes,:]\n",
    "  val_output = np.array(train_output_)[v_indexes,:]\n",
    "\n",
    "  print(len(val_input), len(train_input))\n",
    "\n",
    "  return train_input, train_output, val_input, val_output\n",
    "\n",
    "\n",
    "def format_tr_v(train_input, train_output, val_input, val_output):\n",
    "\n",
    "  '''Prepares the training and validation datasets for frequency analysis, by flattening images.\n",
    "  train_input (samples x W x H): list of input train images\n",
    "  train_output (samples x W x H): list of output train masks\n",
    "  val_input (samples x W x H): list of input val images\n",
    "  val_output (samples x W x H): list of output val masks\n",
    "\n",
    "  return: numpy array, numpy array, numpy array, numpy array\n",
    "  (formatted train input, train output, val input, val output)\n",
    "  '''\n",
    "  \n",
    "  tr_input = np.concatenate([arr.flatten('F')[np.newaxis,:] for arr in train_input],axis=0)\n",
    "  tr_output = np.concatenate([arr.flatten('F')[np.newaxis,:] for arr in train_output],axis=0)\n",
    "  v_input = np.concatenate([arr.flatten('F')[np.newaxis,:] for arr in val_input],axis=0)\n",
    "  v_output = np.concatenate([arr.flatten('F')[np.newaxis,:] for arr in val_output],axis=0)\n",
    "\n",
    "  return tr_input, tr_output, v_input, v_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7fMqRLSCHt3"
   },
   "source": [
    "**STEP 2 - Functions to store and save the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85w9C7r7cFbr"
   },
   "outputs": [],
   "source": [
    "def store_values(tr_output, pca_out, x,  x_sort, F_GT,  v_output, paths):\n",
    "\n",
    "    '''\n",
    "    This function loops over training data files to compute the frequency analysis.\n",
    "\n",
    "    paths: the list of folders (each folder / path = one method like SGD, Adam...) where to find the\n",
    "    DNN training output masks per epoch as .npy\n",
    "\n",
    "    tr_output: train ground truth output masks to compute the train accuracy\n",
    "    pca_out: is the first PC of train output masks\n",
    "    x; Nsamples: is the first projection of training input images on first PC of training input images\n",
    "    x_sort: x values sorted in increasing order\n",
    "    F_GT: non-uniform Fourier coefficients of ground truch function proj(yGT,pca_out) = x (training)\n",
    "    v_output: val ground truth output masks to compute the val accuracy\n",
    "\n",
    "    Return: 6 lists\n",
    "    (paths, Freq errors, val accuracies, traina ccuracies, val epochs, train epochs)\n",
    "\n",
    "    '''\n",
    "\n",
    "    PATH = [] # store the folder path \n",
    "    Ferr = [] # frequency errors per epoch\n",
    "    ACC_V = [] # validation accuracy for each method\n",
    "    ACC_T = [] # training accuracy for each metho\n",
    "    EPS_V = [] # keep track of val epochs order\n",
    "    EPS_T = [] # keep track of train epochs order\n",
    "\n",
    "    for path in paths:\n",
    "\n",
    "          print(path)\n",
    "\n",
    "          epoch_tr, accuracy_tr, F = analize_training_nn(path, tr_output, pca_out, x, x_sort, F_GT, do_FPrinciple = True)\n",
    "          epoch_v, accuracy_v = analize_val(path, v_output)\n",
    "          \n",
    "          PATH.append(path)\n",
    "          Ferr.append(F)\n",
    "          EPS_V.append(epoch_v)\n",
    "          EPS_T.append(epoch_tr)\n",
    "          ACC_V.append(accuracy_v)\n",
    "          ACC_T.append(accuracy_tr)\n",
    "\n",
    "    return PATH, Ferr, ACC_V, ACC_T, EPS_V, EPS_T\n",
    "\n",
    "\n",
    "def save_summaries(PATH, Ferr, ACC_V, ACC_T, EPS_V, EPS_T):\n",
    "\n",
    "  '''Save the results stored in the input parameters as .npy.\n",
    "\n",
    "    PATH: folder paths \n",
    "    Ferr: frequency errors per epoch\n",
    "    ACC_V: validation accuracy for each method\n",
    "    ACC_T: training accuracy for each method\n",
    "    EPS_V: keep track of val epochs order\n",
    "    EPS_T: keep track of train epochs order\n",
    "\n",
    "  '''\n",
    "\n",
    "  for k in range(len(PATH)):\n",
    "\n",
    "    path = PATH[k]\n",
    "    F = Ferr[k]\n",
    "    epoch_tr = EPS_T[k]\n",
    "    accuracy_v = ACC_V[k]\n",
    "    epoch_v = EPS_V[k]\n",
    "    accuracy_tr = ACC_T[k]\n",
    "\n",
    "    savepath = path.replace('Results/','Results/Summary_per_seed/')\n",
    "    print(savepath)\n",
    "\n",
    "    if not os.path.exists(savepath):\n",
    "      os.makedirs(savepath)\n",
    "\n",
    "    np.save(savepath+'FP.npy',F)\n",
    "    np.save(savepath+'epoch_tr.npy',epoch_tr)\n",
    "    np.save(savepath+'accuracy_tr.npy',accuracy_tr)\n",
    "    np.save(savepath+'epoch_v.npy',epoch_v)\n",
    "    np.save(savepath+'accuracy_v.npy',accuracy_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCBkM7IFEaAo"
   },
   "source": [
    "**STEP 3 - Run the analysis for all the methods and store the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "L6F6lTLHm0D3",
    "outputId": "91216286-0857-4d1f-e654-ff52c8deb063"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 699\n",
      "/content/drive/MyDrive/OptML_Data/Results/Logistic_loss epochs=100 val seed = 0/Unet_SCRN(r=5)_batch=5/\n",
      "train_epoch=0.npy\n",
      "train_epoch=1.npy\n",
      "train_epoch=2.npy\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 441\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    442\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f2b8a07c365d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;31m# Run the analysis for the given seed and methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m   \u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFerr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACC_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mACC_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPS_V\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPS_T\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstore_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mx_sort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_GT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m   \u001b[0;31m#save_summaries(PATH, Ferr, ACC_V, ACC_T, EPS_V, EPS_T)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-801fcf506c44>\u001b[0m in \u001b[0;36mstore_values\u001b[0;34m(tr_output, pca_out, x, x_sort, F_GT, v_output, paths)\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           \u001b[0mepoch_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalize_training_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_sort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF_GT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_FPrinciple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m           \u001b[0mepoch_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalize_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-adefbaf7bede>\u001b[0m in \u001b[0;36manalize_training_nn\u001b[0;34m(path, tr_output, pca_out, x, x_sort, F_GT, do_FPrinciple)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# We compute the segmentation accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                 raise IOError(\n\u001b[0;32m--> 451\u001b[0;31m                     \"Failed to interpret file %s as a pickle\" % repr(file)) from e\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;31m# set-up context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0mfixed_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexc_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mexc_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__context__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_ctx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *exc_details)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mis_sync\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mexc_details\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m                     \u001b[0msuppressed_exc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m                     \u001b[0mpending_raise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m_exit_wrapper\u001b[0;34m(exc_type, exc, tb)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_create_exit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm_exit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_exit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcm_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_exit_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 107] Transport endpoint is not connected"
     ]
    }
   ],
   "source": [
    "mainpath = './Results/Logistic_loss epochs=100 val seed = 0'\n",
    "\n",
    "paths = [mainpath+'/Unet_SCRN(r=5)_batch=5/',\n",
    "mainpath+'/Unet_SGD_batch=5/',\n",
    "mainpath+'/Unet_AdaHessian_batch=5/',\n",
    "mainpath+'/Unet_Adam_batch=5/']\n",
    "\n",
    "\n",
    "train_input_, train_output_ = load_data()\n",
    "SEEDS = [0,1,2,3,4] \n",
    "\n",
    "\n",
    "for SEED in SEEDS:\n",
    "\n",
    "  # We load the reference data\n",
    "  train_input, train_output, val_input, val_output = split_train_val(train_input_, SEED=SEED)\n",
    "  tr_input, tr_output, v_input, v_output = format_tr_v(train_input, train_output, val_input, val_output)\n",
    "\n",
    "  # We extract the ground truth Fourier transform\n",
    "  pca_in, x = get_pca_coeffs(tr_input)\n",
    "  pca_out, y = get_pca_coeffs(tr_output)\n",
    "  y_sort = y[np.argsort(x)]\n",
    "  x_sort = np.sort(x)\n",
    "  F_GT = nfft.nfft_adjoint(x_sort,y_sort,20)\n",
    "\n",
    "  # correct the base path to adapt the seed\n",
    "  paths_ = [p.replace('seed = 0','seed = '+str(SEED)) for p in paths]\n",
    "\n",
    "  # Run the analysis for the given seed and methods\n",
    "  PATH, Ferr, ACC_V, ACC_T, EPS_V, EPS_T = store_values(tr_output, pca_out, x,  x_sort, F_GT, v_output, paths_)\n",
    "  save_summaries(PATH, Ferr, ACC_V, ACC_T, EPS_V, EPS_T)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Frequency analysis for HC18 segmentation task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
