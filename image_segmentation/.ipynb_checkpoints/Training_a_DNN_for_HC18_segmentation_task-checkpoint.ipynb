{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYrGOs-3_RHB"
   },
   "source": [
    "### **Training a DNN for HC18 segmentation task**\n",
    "\n",
    "**Description:** In this notebook, we provide the codes for:\n",
    "\n",
    "1.   Opening data and preparing data for training (Functions)\n",
    "2.   Model architecture (Class)\n",
    "3.   Training function\n",
    "4.   Second-order optimizers (Class)\n",
    "5.   Helpers to train with each optimizer and save\n",
    "6.   Utilities to ensure reproductibility\n",
    "7.   Training and save loop over seeds and optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOgNH7utIkD-"
   },
   "source": [
    "**STEP 1 - Opening data and preparing data for training (Functions)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f7q6ZgJuIfqf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def extract_files(path):\n",
    "  ''' Function to help load data. Performs basic pre-processing on images (normalization, masks to float)\n",
    "  path: folder where the training imags are stored\n",
    "\n",
    "  return: list, list (two lists containing the images and masks respectively)\n",
    "  '''\n",
    "\n",
    "  input = []\n",
    "  output = []\n",
    "\n",
    "  def format_mask(mask):\n",
    "    return (mask>0).astype(float)\n",
    "\n",
    "  def normalize(im):\n",
    "    im = im.astype(float)\n",
    "    im = (im - np.min(im))/(np.max(im)-np.min(im))\n",
    "    return im    \n",
    "\n",
    "  for file in os.listdir(path):\n",
    "    if not '_Annotation' in file:\n",
    "\n",
    "      imp = normalize(np.array(PIL.Image.open(path+file)))\n",
    "      imo = format_mask(np.array(PIL.Image.open(path+file.replace('.png','_Annotation.png'))))\n",
    "\n",
    "      input.append(imp)\n",
    "      output.append(imo)\n",
    "\n",
    "  return input, output\n",
    "\n",
    "\n",
    "def load_data(trpath = './Data/format_train/', \n",
    "              vpath = './Data/format_val/'):\n",
    "  \n",
    "  '''This function loads the validation and training data from their respective folders and fuse them to allow the \n",
    "  automatic change of train / val split depending on random shuffling seed. \n",
    "\n",
    "  return: numpy array, numpy array (two numpy arrays with the fused images data and masks respectively)\n",
    "  '''\n",
    "\n",
    "  train_input, train_output = extract_files(trpath)\n",
    "  val_input, val_output = extract_files(vpath)\n",
    "\n",
    "  train_input, train_output = extract_files(trpath)\n",
    "  plus_input, plus_output = extract_files(vpath)\n",
    "\n",
    "  train_input_ = list(np.concatenate([train_input, plus_input]))\n",
    "  train_output_ = list(np.concatenate([train_output, plus_output]))\n",
    "\n",
    "  return train_input_, train_output_\n",
    "\n",
    "\n",
    "def split_train_val(train_input_, SEED=0):\n",
    "\n",
    "  '''This function simply split train_input_ into 300 validation samples and 699 \n",
    "  training samples after random shuffling with seed SEED. \n",
    "\n",
    "  Return: numpy array, numpy array, numpy array \n",
    "  (train images, train masks, val images, val masks resp.)\n",
    "  '''\n",
    "\n",
    "  indexes = np.arange(len(train_input_))\n",
    "  np.random.seed(SEED)\n",
    "  np.random.shuffle(indexes)\n",
    "\n",
    "  v_indexes = indexes[:300]\n",
    "  tr_indexes = indexes[300:]\n",
    "\n",
    "  train_input = np.array(train_input_)[tr_indexes,:]\n",
    "  train_output = np.array(train_output_)[tr_indexes,:]\n",
    "\n",
    "  val_input = np.array(train_input_)[v_indexes,:]\n",
    "  val_output = np.array(train_output_)[v_indexes,:]\n",
    "\n",
    "  print(len(val_input), len(train_input))\n",
    "\n",
    "  return train_input, train_output, val_input, val_output\n",
    "\n",
    "\n",
    "class MyDataset(TensorDataset):\n",
    "  #This class formats the data for training in pytorch.\n",
    "    def __init__(self, data, targets):    \n",
    "      # data: input images\n",
    "      # target: output masks \n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "            x_arr = (self.data[index]).astype(float)\n",
    "            y_arr = (self.targets[index]).astype(float)           \n",
    "            x = torch.Tensor(x_arr[np.newaxis,:,:])\n",
    "            y = torch.Tensor(y_arr).long()\n",
    "            return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "def get_dataloaders(train_input, train_output, val_input, val_output, BATCH_SIZE = 5):\n",
    "\n",
    "  '''This function creates data loaders for training in pytorch. \n",
    "  It also extracts\n",
    "  the weights for cross-entropy loss.\n",
    "  train_input (samples x W x H): list of input train images\n",
    "  train_output (samples x W x H): list of output train masks\n",
    "  val_input (samples x W x H): list of input val images\n",
    "  val_output (samples x W x H): list of output val masks\n",
    "\n",
    "  return : torch dataloader, torch dataloader, numpy array\n",
    "  (training loader, val loader, weights)\n",
    "  '''\n",
    "\n",
    "  # We compute the weights for the Cross Entropy loss\n",
    "  all_pixs = np.concatenate(np.concatenate(train_output))\n",
    "  props = np.sum(all_pixs > 0)/len(all_pixs)\n",
    "  w = 1/np.array([1-props, props])\n",
    "\n",
    "  # create loaders\n",
    "  train_data = MyDataset(train_input, train_output)\n",
    "  train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=False) \n",
    "  valid_data = MyDataset(val_input, val_output)\n",
    "  valid_dl = DataLoader(valid_data, shuffle=False) \n",
    "\n",
    "  return train_dl, valid_dl, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-daVfl5l_Fsb"
   },
   "source": [
    "**STEP 2 - Model architecture (Class)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yi7UAzHA-Wob"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class UNET(nn.Module):\n",
    "    \n",
    "    # Sandra Marcadent pytorch implementation of 2D Unet [1]\n",
    "\n",
    "    # [1] @article{Ronneberger2015, \n",
    "    # author = {Olaf Ronneberger and Philipp Fischer and Thomas Brox}, \n",
    "    # month = {5}, \n",
    "    # title = {U-Net: Convolutional Networks for Biomedical Image # Segmentation},\n",
    "    # url = {http://arxiv.org/abs/1505.04597},\n",
    "    # year = {2015},\n",
    "    #}\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv1 = self.contract_block(in_channels, 64, 3, 1)\n",
    "        self.conv2 = self.contract_block(64, 128, 3, 1)\n",
    "        self.conv3 = self.contract_block(128, 256, 3, 1)\n",
    "        self.conv4 = self.contract_block(256, 512, 3, 1)\n",
    "        self.conv5 = self.contract_block(512, 1024, 3, 1)\n",
    "        \n",
    "        self.upconv5 = self.expand_block(1024, 512, 3, 1)\n",
    "        self.upconv4 = self.expand_block(512*2, 256, 3, 1)\n",
    "        self.upconv3 = self.expand_block(256*2, 128, 3, 1)\n",
    "        self.upconv2 = self.expand_block(128*2, 64, 3, 1)\n",
    "        self.upconv1 = self.expand_block(64*2, self.out_channels, 3, 1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # downsampling part\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "        conv4 = self.conv4(conv3)\n",
    "        conv5 = self.conv5(conv4)\n",
    "\n",
    "        upconv5 = self.upconv5(conv5)\n",
    "        \n",
    "        upconv4 = self.upconv4(torch.cat([upconv5, conv4], 1))\n",
    "        upconv3 = self.upconv3(torch.cat([upconv4, conv3], 1))\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "\n",
    "        return upconv1\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        contract = nn.Sequential(\n",
    "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            torch.nn.BatchNorm2d(out_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "                                 )\n",
    "\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "\n",
    "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "                            torch.nn.BatchNorm2d(out_channels),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2, padding=0, output_padding=0))\n",
    "        return expand\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4SlATX3_IiF"
   },
   "source": [
    "**STEP 3 - Training function** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FK0CGD_V-khA"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "def train(model, device, train_dl, valid_dl, loss_fn, optimizer, epochs=100,\n",
    "          lr_base=0.01, save=False, savepath='', mode=''):\n",
    "  \n",
    "    '''This function trains a DNN and saves the results for frequency analysis.\n",
    "    model: torch.nn network\n",
    "    device: gpu or cpu torch device\n",
    "    train_dl: training dataloader\n",
    "    valid_dl: validation dataloader\n",
    "    loss_fn: loss function to optimize\n",
    "    mode: Hessian or empty (default) mode\n",
    "\n",
    "    return: torch.nn \n",
    "    (The trained neural network)\n",
    "    '''\n",
    "    \n",
    "    start = time.time()\n",
    "    model.to(device, non_blocking=False)\n",
    "    loss_fn = loss_fn.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        train_outputs = []\n",
    "        val_outputs = []\n",
    "\n",
    "        for phase in ['train', 'valid']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set training mode = true\n",
    "                dataloader = train_dl\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "                dataloader = valid_dl\n",
    "\n",
    "\n",
    "            for step, data in enumerate(dataloader):\n",
    "                \n",
    "                x,y = data\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    " \n",
    "                if phase == 'train':\n",
    "                    if mode == 'Hessian':\n",
    "                          def closure():\n",
    "\n",
    "                              outputs = model(x)\n",
    "                              optimizer.zero_grad()\n",
    "                              loss = loss_fn(outputs, y)\n",
    "                              loss.backward(create_graph=True) #needed to compute the Hessian \n",
    "                              return loss\n",
    "          \n",
    "                          optimizer.step(closure)\n",
    "\n",
    "                          with torch.no_grad():  \n",
    "                              outputs = model(x)\n",
    "                              loss = loss_fn(outputs, y)\n",
    "\n",
    "                    else:\n",
    "                              optimizer.zero_grad()\n",
    "                              outputs = model(x)\n",
    "                              loss = loss_fn(outputs, y)\n",
    "                              loss.backward()\n",
    "                              optimizer.step()\n",
    "\n",
    "                        \n",
    "                    if save:\n",
    "                            y_hat_ = get_outputs(outputs) # format the outputs to be numpy arrays\n",
    "                            train_outputs.append(y_hat_)\n",
    "\n",
    "                    if step % 10 == 0:\n",
    "                        print(phase, ' --- Current step: {}  Loss: {}  AllocMem (Mb): {}'.format(step, loss,\n",
    "                                                                    torch.cuda.memory_allocated()/1024/1024))\n",
    "\n",
    "\n",
    "                else:    \n",
    "                        with torch.no_grad():\n",
    "\n",
    "                          outputs = model(x)\n",
    "                          loss = loss_fn(outputs, y)\n",
    "\n",
    "                          # loss visu to check that the training goes well\n",
    "                          if epoch%2 == 0:   \n",
    "                                if step%10 == 0:\n",
    "                                    print(phase, ' --- Current step: {}  Loss: {}  AllocMem (Mb): {}'.format(step, loss,\n",
    "                                                                        torch.cuda.memory_allocated()/1024/1024))\n",
    "\n",
    "                          if save:\n",
    "                            y_hat_ = get_outputs(outputs, val=True) # format the outputs to be numpy arrays\n",
    "                            val_outputs.append(y_hat_) # already formatted for frequency analysis\n",
    "\n",
    "\n",
    "\n",
    "        if save:\n",
    "          # Save the results for further analysis\n",
    "            val_outputs = np.concatenate(val_outputs, axis = 0)\n",
    "            train_outputs = np.concatenate(train_outputs, axis = 0)\n",
    "            np.save(savepath+'val_epoch='+str(epoch)+'.npy',val_outputs)\n",
    "            np.save(savepath+'train_epoch='+str(epoch)+'.npy',train_outputs)\n",
    "\n",
    "               \n",
    "    x = x.cpu()\n",
    "    y = y.cpu()\n",
    "    loss_fn = loss_fn.cpu()\n",
    "    model = model.cpu()\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def get_outputs(y_hat, val=False):\n",
    "\n",
    "  '''This function formats the DNN output for frequency analysis.\n",
    "  y_hat: a mini batch of size (B x C x H x W)\n",
    "\n",
    "  return: numpy array\n",
    "  (formated y_hat)\n",
    "  '''\n",
    "\n",
    "  yh = torch.argmax(y_hat.float(),dim=1).float()\n",
    "  img = yh.cpu().numpy()\n",
    "  img = np.transpose(img,(1, 2, 0))\n",
    "\n",
    "  if val: # if val we then have a batchsize of one\n",
    "    img_ = img[:,:,0].flatten('F')\n",
    "    return img_[np.newaxis,:]\n",
    "\n",
    "  outputs = []\n",
    "\n",
    "  for k in range(img.shape[2]): # run over the batch to accumulate the outputs\n",
    "    img_ = img[:,:,k].flatten('F')\n",
    "    outputs.append(img_[np.newaxis,:])\n",
    "\n",
    "  return np.concatenate(outputs,axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97se2XC-LB8u"
   },
   "source": [
    "**STEP 4 - Second-order optimizers (Class)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJrzwIGpNiZd"
   },
   "source": [
    "AdaHessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "pITzpCoHJAVD"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "from pdb import set_trace as bp\n",
    "\n",
    "\n",
    "class AdaHessian(torch.optim.Optimizer):\n",
    "    \"\"\"\n",
    "    Implements the AdaHessian algorithm from \"ADAHESSIAN: An Adaptive Second OrderOptimizer for Machine Learning\"\n",
    "    Arguments:\n",
    "        params (iterable) -- iterable of parameters to optimize or dicts defining parameter groups\n",
    "        lr (float, optional) -- learning rate (default: 0.1)\n",
    "        betas ((float, float), optional) -- coefficients used for computing running averages of gradient and the squared hessian trace (default: (0.9, 0.999))\n",
    "        eps (float, optional) -- term added to the denominator to improve numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional) -- weight decay (L2 penalty) (default: 0.0)\n",
    "        hessian_power (float, optional) -- exponent of the hessian trace (default: 1.0)\n",
    "        update_each (int, optional) -- compute the hessian trace approximation only after *this* number of steps (to save time) (default: 1)\n",
    "        n_samples (int, optional) -- how many times to sample `z` for the approximation of the hessian trace (default: 1)\n",
    "\n",
    "    Reference: https://github.com/amirgholami/adahessian\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.1, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.0, \n",
    "                 hessian_power=1.0, update_each=1, n_samples=1, average_conv_kernel=False):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(f\"Invalid epsilon value: {eps}\")\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 0: {betas[0]}\")\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(f\"Invalid beta parameter at index 1: {betas[1]}\")\n",
    "        if not 0.0 <= hessian_power <= 1.0:\n",
    "            raise ValueError(f\"Invalid Hessian power value: {hessian_power}\")\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "        self.update_each = update_each\n",
    "        self.average_conv_kernel = average_conv_kernel\n",
    "\n",
    "        # use a separate generator that deterministically generates the same `z`s across all GPUs in case of distributed training\n",
    "        self.generator = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, hessian_power=hessian_power)\n",
    "        super(AdaHessian, self).__init__(params, defaults)\n",
    "\n",
    "        for p in self.get_params():\n",
    "            p.hess = 0.0\n",
    "            self.state[p][\"hessian step\"] = 0\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Gets all parameters in all param_groups with gradients\n",
    "        \"\"\"\n",
    "\n",
    "        return (p for group in self.param_groups for p in group['params'] if p.requires_grad)\n",
    "\n",
    "    def zero_hessian(self):\n",
    "        \"\"\"\n",
    "        Zeros out the accumalated hessian traces.\n",
    "        \"\"\"\n",
    "\n",
    "        for p in self.get_params():\n",
    "            if not isinstance(p.hess, float) and self.state[p][\"hessian step\"] % self.update_each == 0:\n",
    "                p.hess.zero_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def set_hessian(self):\n",
    "        \"\"\"\n",
    "        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        params = []\n",
    "        for p in filter(lambda p: p.grad is not None, self.get_params()):\n",
    "            if self.state[p][\"hessian step\"] % self.update_each == 0:  # compute the trace only each `update_each` step\n",
    "                params.append(p)\n",
    "            self.state[p][\"hessian step\"] += 1\n",
    "\n",
    "        if len(params) == 0:\n",
    "            return\n",
    "\n",
    "        if self.generator.device != params[0].device:  # hackish way of casting the generator to the right device\n",
    "            self.generator = torch.Generator(params[0].device).manual_seed(2147483647)\n",
    "\n",
    "        grads = [p.grad for p in params]\n",
    "\n",
    "        for i in range(self.n_samples):\n",
    "            zs = [torch.randint(0, 2, p.size(), generator=self.generator, device=p.device) * 2.0 - 1.0 for p in params]  # Rademacher distribution {-1.0, 1.0}\n",
    "            h_zs = torch.autograd.grad(grads, params, grad_outputs=zs, only_inputs=True, retain_graph=i < self.n_samples - 1)\n",
    "            for h_z, z, p in zip(h_zs, zs, params):\n",
    "                p.hess += h_z * z / self.n_samples  # approximate the expected values of z*(H@z)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"\n",
    "        Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional) -- a closure that reevaluates the model and returns the loss (default: None)\n",
    "        \"\"\"\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        self.zero_hessian()\n",
    "        self.set_hessian()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None or p.hess is None:\n",
    "                    continue\n",
    "\n",
    "                if self.average_conv_kernel and p.dim() == 4:\n",
    "                    p.hess = torch.abs(p.hess).mean(dim=[2, 3], keepdim=True).expand_as(p.hess).clone()\n",
    "\n",
    "                # Perform correct stepweight decay as in AdamW\n",
    "                p.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 1:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data)  # Exponential moving average of gradient values\n",
    "                    state['exp_hessian_diag_sq'] = torch.zeros_like(p.data)  # Exponential moving average of Hessian diagonal square values\n",
    "\n",
    "                exp_avg, exp_hessian_diag_sq = state['exp_avg'], state['exp_hessian_diag_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                state['step'] += 1\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                exp_avg.mul_(beta1).add_(p.grad, alpha=1 - beta1)\n",
    "                exp_hessian_diag_sq.mul_(beta2).addcmul_(p.hess, p.hess, value=1 - beta2)\n",
    "\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                k = group['hessian_power']\n",
    "                denom = (exp_hessian_diag_sq / bias_correction2).pow_(k / 2).add_(group['eps'])\n",
    "\n",
    "                # make update\n",
    "                step_size = group['lr'] / bias_correction1\n",
    "                p.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEVhib8FNfsE"
   },
   "source": [
    "SCRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-Jq9Tu7MfJ4v"
   },
   "outputs": [],
   "source": [
    "class SCRNOptimizer(Optimizer):\n",
    "    def __init__(self, params, inner_itr=10, ro=0.1, l=0.5, epsilon=1e-3, c_prime=0.1, step_size=0.001):\n",
    "\n",
    "        self.ro = ro\n",
    "        self.l = l\n",
    "        self.epsilon = epsilon\n",
    "        self.c_prime = c_prime\n",
    "        self.inner_itr = inner_itr\n",
    "        self.step_size = 1 / (20 * l)\n",
    "        self.iteration = -1\n",
    "        defaults = dict()\n",
    "        self.sqr_grads_norms = 0\n",
    "        self.last_grad_norm = 0\n",
    "\n",
    "        super(SCRNOptimizer, self).__init__(params, defaults)\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                state['displacement'] = torch.zeros_like(p)\n",
    "\n",
    "    def compute_norm_of_list_var(self, array_):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        param array_: list of tensors\n",
    "        return:\n",
    "        norm of the flattened list\n",
    "        \"\"\"\n",
    "        norm_square = 0\n",
    "        for i in range(len(array_)):\n",
    "            norm_square += array_[i].norm(2).item() ** 2\n",
    "        return norm_square ** 0.5\n",
    "\n",
    "    def inner_product_of_list_var(self, array1_, array2_):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        param array1_: list of tensors\n",
    "        param array2_: list of tensors\n",
    "        return:\n",
    "        The inner product of the flattened list\n",
    "        \"\"\"\n",
    "\n",
    "        sum_list = 0\n",
    "        for i in range(len(array1_)):\n",
    "            sum_list += torch.sum(array1_[i] * array2_[i])\n",
    "        return sum_list\n",
    "\n",
    "    def cubic_subsolver(self, grads, param, grad_norm: float, epsilon: float, ro: float, l: float):\n",
    "        \"\"\"\n",
    "        solve the sub problem with gradient decent\n",
    "        \"\"\"\n",
    "        deltas = [0] * len(grads)\n",
    "        g_tildas = [0] * len(grads)\n",
    "\n",
    "        # compute the hessian\n",
    "        #hessian = torch.autograd.grad(outputs=grads, inputs=param, retain_graph=True)\n",
    "        # turn of unwanted actions\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if grad_norm >= l ** 2 / self.ro:\n",
    "\n",
    "                # compute hessian vector with respect to grads\n",
    "                hvp = torch.autograd.grad(outputs=grads, inputs=param,\n",
    "                                          grad_outputs=grads, retain_graph=True)\n",
    "                \n",
    "                g_t_dot_bg_t = self.inner_product_of_list_var(grads, hvp) / (ro * (grad_norm ** 2))\n",
    "                R_c = -g_t_dot_bg_t + (g_t_dot_bg_t ** 2 + 2 * grad_norm / ro) ** 0.5\n",
    "\n",
    "                for i in range(len(grads)):\n",
    "                    deltas[i] = -R_c * grads[i].clone() / grad_norm\n",
    "\n",
    "            else:\n",
    "                sigma = self.c_prime * (epsilon * ro) ** 0.5 / l\n",
    "                for i in range(len(grads)):\n",
    "                    deltas[i] = torch.zeros(grads[i].shape)\n",
    "                    khi = torch.rand(grads[i].shape)\n",
    "                    g_tildas[i] = grads[i].clone() + sigma * khi\n",
    "                for t in range(self.inner_itr):\n",
    "                    # compute hessian vector with respect to delta\n",
    "                    hvp = torch.autograd.grad(outputs=grads, inputs=param,\n",
    "                                              grad_outputs=deltas, retain_graph=True)\n",
    "                    deltas_norm = self.compute_norm_of_list_var(deltas)\n",
    "                    if self.compute_norm_of_list_var(hvp)>200:\n",
    "                        break\n",
    "\n",
    "                    for i in range(len(grads)):\n",
    "                        deltas[i] = deltas[i] - self.step_size * (\n",
    "                                g_tildas[i] + hvp[i] + ro / 2 * deltas_norm * deltas[i])\n",
    "                    # print(\"*********************\")\n",
    "                    # print(deltas[0])\n",
    "                    # print(self.step_size)\n",
    "                    # print(g_tildas[0])\n",
    "                    # print(hvp[0])\n",
    "                    # print(deltas_norm)\n",
    "\n",
    "        # compute hessian vector with respect to delta\n",
    "        hvp = torch.autograd.grad(outputs=grads, inputs=param,\n",
    "                                  grad_outputs=deltas, retain_graph=True)\n",
    "        deltas_norm = self.compute_norm_of_list_var(deltas)\n",
    "        delta_m = 0\n",
    "        for i in range(len(grads)):\n",
    "            delta_m += torch.sum(grads[i] * deltas[i]) + 0.5 * torch.sum(deltas[i] * hvp[i]) + ro / 6 * deltas_norm ** 3\n",
    "\n",
    "        deltas_norm = 0\n",
    "        # update the displacement\n",
    "        for group in self.param_groups:\n",
    "            i = 0\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                deltas_norm += deltas[i].norm(2).item() ** 2\n",
    "                state['displacement'] = deltas[i]\n",
    "                i += 1\n",
    "\n",
    "        return delta_m.item(), deltas_norm ** 0.5\n",
    "\n",
    "    def cubic_finalsolver(self, grads, param, epsilon: float, ro: float, l: float):\n",
    "        \"\"\"\n",
    "        solve the sub problem with gradient decent\n",
    "        \"\"\"\n",
    "        grads_m = [0] * len(grads)\n",
    "        with torch.no_grad():\n",
    "            deltas = [0] * len(grads)\n",
    "            for i in range(len(grads)):\n",
    "                deltas[i] = torch.zeros_like(grads[i])\n",
    "                grads_m[i] = grads[i].clone()\n",
    "            while self.compute_norm_of_list_var(grads_m, ) > epsilon / 2:\n",
    "                hvp = torch.autograd.grad(outputs=grads, inputs=param, grad_outputs=deltas, retain_graph=True)\n",
    "                for i in range(len(grads)):\n",
    "                    deltas[i] = deltas[i] - self.step_size * grads_m[i]\n",
    "                deltas_norm = self.compute_norm_of_list_var(deltas)\n",
    "                for i in range(len(grads)):\n",
    "                    grads_m[i] = grads[i] + hvp[i] + ro / 2 * deltas_norm * deltas[i]\n",
    "\n",
    "            # update the displacement\n",
    "            for group in self.param_groups:\n",
    "                with torch.no_grad():\n",
    "                    i = 0\n",
    "                    for p in group['params']:\n",
    "                        state = self.state[p]\n",
    "                        state['displacement'] = deltas[i]\n",
    "                        i += 1\n",
    "\n",
    "    def update_parameters(self, ):\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            with torch.no_grad():\n",
    "                for p in group['params']:\n",
    "                    state = self.state[p]\n",
    "                    displacement = state['displacement']\n",
    "                    p.add_(displacement.clone())\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Args:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "        self.iteration += 1\n",
    "\n",
    "        # compute the gradiant\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            grads = []\n",
    "            param = []\n",
    "            grad_square_norm = 0\n",
    "\n",
    "            for p in group['params']:\n",
    "\n",
    "                    with torch.no_grad():\n",
    "\n",
    "                        d_p = p.grad\n",
    "                        grad_square_norm += d_p.norm(2).item() ** 2\n",
    "\n",
    "                    grads.append(p.grad)\n",
    "                    param.append(p)\n",
    "\n",
    "            # hessian = self.eval_hessian(grads,param)\n",
    "            # print(hessian.shape)\n",
    "            # e, _ = torch.eig(torch.tensor(hessian))\n",
    "            # lambda_min = torch.min(e[:, 0]).item()\n",
    "            delta_m, deltas_norm = self.cubic_subsolver(grads, param, grad_square_norm ** 0.5, self.epsilon, self.ro,\n",
    "                                                        self.l)\n",
    "            # if delta_m >= -(self.epsilon ** 3 / self.ro) ** 0.5 / 100:\n",
    "            #     self.cubic_finalsolver(grads, param, self.epsilon, self.ro, self.l)\n",
    "            #     self.update_parameters()\n",
    "            #     return loss\n",
    "            # else:\n",
    "\n",
    "            self.update_parameters()\n",
    "\n",
    "            # with tabular.prefix(\"SCRN\" + '/'):\n",
    "            #     tabular.record('delta of m', delta_m)\n",
    "            #     tabular.record('norm of gradient', grad_square_norm ** (1. / 2))\n",
    "            #     tabular.record('norm of deltas', deltas_norm)\n",
    "            #     # tabular.record('landa min', lambda_min)\n",
    "            #     logger.log(tabular)\n",
    "        return loss\n",
    "\n",
    "    # eval Hessian matrix\n",
    "    def eval_hessian(self, loss_grad, params):\n",
    "        cnt = 0\n",
    "        for g in loss_grad:\n",
    "            g_vector = g.contiguous().view(-1) if cnt == 0 else torch.cat([g_vector, g.contiguous().view(-1)])\n",
    "            cnt = 1\n",
    "        l = g_vector.size(0)\n",
    "        hessian = torch.zeros(l, l)\n",
    "        for idx in range(l):\n",
    "            grad2rd = torch.autograd.grad(g_vector[idx], params, create_graph=True)\n",
    "            cnt = 0\n",
    "            for g in grad2rd:\n",
    "                g2 = g.contiguous().view(-1) if cnt == 0 else torch.cat([g2, g.contiguous().view(-1)])\n",
    "                cnt = 1\n",
    "            hessian[idx] = g2\n",
    "        return hessian.cpu().data.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xLm8Ul9LU3f"
   },
   "source": [
    "**STEP 5 - Helpers to train with each optimizer and save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uPV3X8IufPgG"
   },
   "outputs": [],
   "source": [
    "def train_Adam(device,\n",
    "               train_dl, \n",
    "               valid_dl,\n",
    "                w,\n",
    "                LR_BASE = 0.01,\n",
    "                savepath='./Results/Logistic_loss epochs=100 val seed = 0/Unet_Adam_batch=5/',\n",
    "                SEED=0):\n",
    "  \n",
    "    '''Trains a unet model with Adam and weighted cross-entropy loss. Saves the results for frequency analysis\n",
    "    in a newly created folder which corresponds to the optimization method. \n",
    "    device: gpu or cpu torch device\n",
    "      train_dl: training dataloader\n",
    "      valid_dl: validation dataloader\n",
    "      w: weights for the cross-entropy loss\n",
    "      LR_BASE: learning rate\n",
    "      SEED: the seed used to initialize the model and split the dataset (only used for the savepath here)\n",
    "    '''\n",
    "\n",
    "    savepath_ = savepath.replace('seed = 0','seed = '+str(SEED)) # adapt the savepath with the seed\n",
    "    print(savepath_)\n",
    "    if not os.path.exists(savepath_): # create the folder\n",
    "      os.makedirs(savepath_)\n",
    "\n",
    "    unet = UNET(1,2)\n",
    "    opt = torch.optim.Adam(unet.parameters(),lr=LR_BASE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(w))\n",
    "    model = train(unet, device, train_dl, valid_dl, loss_fn, opt, epochs=100,\n",
    "                  lr_base = LR_BASE, save=True, savepath=savepath_)\n",
    "\n",
    "    torch.cuda.empty_cache() #empty gpu\n",
    "\n",
    "\n",
    "def train_SGD(train_dl, \n",
    "               valid_dl,\n",
    "                w,\n",
    "                LR_BASE = 0.01,\n",
    "                savepath='./Results/Logistic_loss epochs=100 val seed = 0/Unet_SGD_batch=5/',\n",
    "                SEED=0):\n",
    "\n",
    "    '''Trains a unet model with SGD and weighted cross-entropy loss. Saves the results for frequency analysis\n",
    "    in a newly created folder which corresponds to the optimization method. \n",
    "    device: gpu or cpu torch device\n",
    "      train_dl: training dataloader\n",
    "      valid_dl: validation dataloader\n",
    "      w: weights for the cross-entropy loss\n",
    "      LR_BASE: learning rate\n",
    "      SEED: the seed used to initialize the model and split the dataset (only used for the savepath here)\n",
    "    '''\n",
    "\n",
    "    savepath_ = savepath.replace('seed = 0','seed = '+str(SEED))\n",
    "    print(savepath_)\n",
    "\n",
    "    if not os.path.exists(savepath_): # create the folder\n",
    "      os.makedirs(savepath_)\n",
    "\n",
    "    unet = UNET(1,2)\n",
    "    opt = torch.optim.SGD(unet.parameters(),lr=LR_BASE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(w))\n",
    "    model = train(unet, device, train_dl, valid_dl, loss_fn, opt, epochs=100,\n",
    "                  lr_base = LR_BASE, save=True, savepath=savepath_)\n",
    "\n",
    "    torch.cuda.empty_cache() #empty gpu\n",
    "\n",
    "\n",
    "def train_SCRN(train_dl, \n",
    "               valid_dl,\n",
    "                w,\n",
    "                LR_BASE = 0.01,\n",
    "                savepath='./Results/Logistic_loss epochs=100 val seed = 0/Unet_SCRN(r=5)_batch=5/',\n",
    "                SEED=0):\n",
    "\n",
    "\n",
    "    '''Trains a unet model with SCRN and weighted cross-entropy loss. Saves the results for frequency analysis\n",
    "    in a newly created folder which corresponds to the optimization method. \n",
    "    device: gpu or cpu torch device\n",
    "      train_dl: training dataloader\n",
    "      valid_dl: validation dataloader\n",
    "      w: weights for the cross-entropy loss\n",
    "      LR_BASE: learning rate\n",
    "      SEED: the seed used to initialize the model and split the dataset (only used for the savepath here)\n",
    "    '''\n",
    "\n",
    "    savepath_ = savepath.replace('seed = 0','seed = '+str(SEED))\n",
    "    print(savepath_)\n",
    "\n",
    "    if not os.path.exists(savepath_): # create the folder\n",
    "      os.makedirs(savepath_)\n",
    "\n",
    "\n",
    "    unet = UNET(1,2)\n",
    "    opt = SCRNOptimizer(unet.parameters(), l=LR_BASE, ro=5)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(w))\n",
    "    model = train(unet, device, train_dl, valid_dl, loss_fn, opt, epochs=100,\n",
    "                  lr_base = LR_BASE, save=True, savepath=savepath_, mode='Hessian')\n",
    "\n",
    "    torch.cuda.empty_cache() #empty gpu\n",
    "\n",
    "\n",
    "def train_AdaHessian(train_dl, \n",
    "               valid_dl,\n",
    "                w,\n",
    "                LR_BASE = 0.01,\n",
    "                savepath='./Results/Logistic_loss epochs=100 val seed = 0/Unet_AdaHessian_batch=5/',\n",
    "                SEED=0):\n",
    "\n",
    "    '''Trains a unet model with AdaHessian and weighted cross-entropy loss. Saves the results for frequency analysis\n",
    "    in a newly created folder which corresponds to the optimization method. \n",
    "    device: gpu or cpu torch device\n",
    "      train_dl: training dataloader\n",
    "      valid_dl: validation dataloader\n",
    "      w: weights for the cross-entropy loss\n",
    "      LR_BASE: learning rate\n",
    "      SEED: the seed used to initialize the model and split the dataset (only used for the savepath here)\n",
    "    '''\n",
    "\n",
    "    savepath_ = savepath.replace('seed = 0','seed = '+str(SEED))\n",
    "    print(savepath_)\n",
    "\n",
    "    if not os.path.exists(savepath_): # create the folder\n",
    "      os.makedirs(savepath_)\n",
    "\n",
    "    unet = UNET(1,2)\n",
    "    opt = AdaHessian(unet.parameters(), lr=LR_BASE)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(weight=torch.Tensor(w))\n",
    "\n",
    "    model = train(unet, device, train_dl, valid_dl, loss_fn, opt, epochs=100,\n",
    "                  lr_base = LR_BASE, save=True, savepath=savepath_, mode='Hessian')\n",
    "    torch.cuda.empty_cache() #empty gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gg4_nougLbDU"
   },
   "source": [
    "**STEP 6 - Utilities to ensure reproductibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WPZsFu7K_ceg"
   },
   "outputs": [],
   "source": [
    "\"\"\"Utilities for ensuring that experiments are deterministic.\"\"\"\n",
    "import random\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "seed_ = None\n",
    "seed_stream_ = None\n",
    "\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set the process-wide random seed.\n",
    "\n",
    "    Args:\n",
    "        seed (int): A positive integer\n",
    "\n",
    "    \"\"\"\n",
    "    seed %= 4294967294\n",
    "    # pylint: disable=global-statement\n",
    "    global seed_\n",
    "    global seed_stream_\n",
    "    seed_ = seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if 'tensorflow' in sys.modules:\n",
    "        import tensorflow as tf  # pylint: disable=import-outside-toplevel\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        try:\n",
    "            # pylint: disable=import-outside-toplevel\n",
    "            import tensorflow_probability as tfp\n",
    "            seed_stream_ = tfp.util.SeedStream(seed_, salt='garage')\n",
    "        except ImportError:\n",
    "            pass\n",
    "    if 'torch' in sys.modules:\n",
    "        warnings.warn(\n",
    "            'Enabeling deterministic mode in PyTorch can have a performance '\n",
    "            'impact when using GPU.')\n",
    "        import torch  # pylint: disable=import-outside-toplevel\n",
    "        torch.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_seed():\n",
    "    \"\"\"Get the process-wide random seed.\n",
    "\n",
    "    Returns:\n",
    "        int: The process-wide random seed\n",
    "\n",
    "    \"\"\"\n",
    "    return seed_\n",
    "\n",
    "\n",
    "def get_tf_seed_stream():\n",
    "    \"\"\"Get the pseudo-random number generator (PRNG) for TensorFlow ops.\n",
    "\n",
    "    Returns:\n",
    "        int: A seed generated by a PRNG with fixed global seed.\n",
    "\n",
    "    \"\"\"\n",
    "    if seed_stream_ is None:\n",
    "        set_seed(0)\n",
    "    return seed_stream_() % 4294967294\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxm_MhYYCj6o"
   },
   "source": [
    "**STEP 7 - Training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqDqKuMTClDX",
    "outputId": "cef38002-1c12-48a9-a3c1-06438a2eae3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# We use GPU resources\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "\n",
    "device = torch.device(['cpu','cuda'][torch.cuda.is_available()])\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "PmvU7PJrRiQ3",
    "outputId": "a72c7907-aa00-4ffa-e9b8-58c38350234e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 699\n",
      "/content/drive/MyDrive/OptML_Data/Results/Logistic_loss epochs=100 val seed = 0/Unet_SGD_batch=5/\n",
      "Epoch 0/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.7322268486022949  AllocMem (Mb): 250.8642578125\n",
      "train  --- Current step: 10  Loss: 0.6976431012153625  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.7056930661201477  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.6738709211349487  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.6912842392921448  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.6731741428375244  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.6902633905410767  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.6783270239830017  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.6764160990715027  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6800898313522339  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.679336428642273  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.6609258055686951  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.6613583564758301  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.6545591950416565  AllocMem (Mb): 250.6142578125\n",
      "valid  --- Current step: 0  Loss: 0.660494863986969  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 10  Loss: 0.6571733951568604  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 20  Loss: 0.6420170664787292  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 30  Loss: 0.635860025882721  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 40  Loss: 0.6616133451461792  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 50  Loss: 0.6569763422012329  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 60  Loss: 0.6556116938591003  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 70  Loss: 0.6334001421928406  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 80  Loss: 0.6687909960746765  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 90  Loss: 0.6546437740325928  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 100  Loss: 0.6583788394927979  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 110  Loss: 0.6658855676651001  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 120  Loss: 0.6606526970863342  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 130  Loss: 0.6874682903289795  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 140  Loss: 0.6450162529945374  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 150  Loss: 0.6441020369529724  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 160  Loss: 0.6394903659820557  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 170  Loss: 0.6513919234275818  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 180  Loss: 0.6794463992118835  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 190  Loss: 0.6255608201026917  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 200  Loss: 0.6384757161140442  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 210  Loss: 0.6590763926506042  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 220  Loss: 0.6587328314781189  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 230  Loss: 0.6486917734146118  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 240  Loss: 0.6449061632156372  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 250  Loss: 0.6661948561668396  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 260  Loss: 0.6520936489105225  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 270  Loss: 0.6660643219947815  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 280  Loss: 0.6477370858192444  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 290  Loss: 0.651130735874176  AllocMem (Mb): 244.8642578125\n",
      "Epoch 1/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.656945526599884  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.6653608679771423  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.6621814370155334  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.6505210995674133  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.6654192805290222  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.652313768863678  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.6554790735244751  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.6673253774642944  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.657682478427887  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6695871949195862  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.6468592882156372  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.6390210390090942  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.6466035842895508  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.6322988867759705  AllocMem (Mb): 250.6142578125\n",
      "Epoch 2/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.6262969374656677  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.6477397084236145  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.6322143077850342  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.627418041229248  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.6429125070571899  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.6315771341323853  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.6267455816268921  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.6544964909553528  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.6395753026008606  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6594278812408447  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.6151304841041565  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.6146907210350037  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.6285746693611145  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.6075069308280945  AllocMem (Mb): 250.6142578125\n",
      "valid  --- Current step: 0  Loss: 0.6173534393310547  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 10  Loss: 0.5841341614723206  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 20  Loss: 0.6047247648239136  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 30  Loss: 0.5209528803825378  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 40  Loss: 0.6301665902137756  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 50  Loss: 0.6160286664962769  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 60  Loss: 0.6123011112213135  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 70  Loss: 0.5103448629379272  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 80  Loss: 0.5948845148086548  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 90  Loss: 0.6194854378700256  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 100  Loss: 0.6246249079704285  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 110  Loss: 0.6375570893287659  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 120  Loss: 0.6281272172927856  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 130  Loss: 0.6707829236984253  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 140  Loss: 0.6139822006225586  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 150  Loss: 0.5882161259651184  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 160  Loss: 0.5590955018997192  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 170  Loss: 0.6044303178787231  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 180  Loss: 0.681411623954773  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 190  Loss: 0.5952233672142029  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 200  Loss: 0.5319920182228088  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 210  Loss: 0.6181138753890991  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 220  Loss: 0.6170582175254822  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 230  Loss: 0.5950027704238892  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 240  Loss: 0.6157526969909668  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 250  Loss: 0.6370570063591003  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 260  Loss: 0.6079331040382385  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 270  Loss: 0.6317778825759888  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 280  Loss: 0.5918723344802856  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 290  Loss: 0.60599684715271  AllocMem (Mb): 244.8642578125\n",
      "Epoch 3/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.5965513586997986  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.6279406547546387  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.6069324016571045  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.6023164391517639  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.618962287902832  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.6087542176246643  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.5989305377006531  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.6359479427337646  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.6202937364578247  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6477513909339905  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.576663613319397  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.583045482635498  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.6060115694999695  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.5802633166313171  AllocMem (Mb): 250.6142578125\n",
      "Epoch 4/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.5643197894096375  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.606154203414917  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.5811209678649902  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.5761530995368958  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.592788577079773  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.5826590657234192  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.5721644759178162  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.6114797592163086  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.5988332033157349  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6356854438781738  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.5440530776977539  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.5528176426887512  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.5810872316360474  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.5509786605834961  AllocMem (Mb): 250.6142578125\n",
      "valid  --- Current step: 0  Loss: 0.5683706998825073  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 10  Loss: 0.5274421572685242  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 20  Loss: 0.5551630258560181  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 30  Loss: 0.4263648986816406  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 40  Loss: 0.5907838940620422  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 50  Loss: 0.5806266069412231  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 60  Loss: 0.5551139116287231  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 70  Loss: 0.41399967670440674  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 80  Loss: 0.5431603789329529  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 90  Loss: 0.5783968567848206  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 100  Loss: 0.5780657529830933  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 110  Loss: 0.5970463156700134  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 120  Loss: 0.586981475353241  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 130  Loss: 0.6476635932922363  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 140  Loss: 0.5611487030982971  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 150  Loss: 0.5233101844787598  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 160  Loss: 0.47891685366630554  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 170  Loss: 0.5472406148910522  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 180  Loss: 0.6684445142745972  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 190  Loss: 0.5357613563537598  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 200  Loss: 0.44953322410583496  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 210  Loss: 0.5697605609893799  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 220  Loss: 0.5688347220420837  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 230  Loss: 0.5343061685562134  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 240  Loss: 0.5661015510559082  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 250  Loss: 0.604845404624939  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 260  Loss: 0.5549786686897278  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 270  Loss: 0.591709554195404  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 280  Loss: 0.5279210805892944  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 290  Loss: 0.5482901930809021  AllocMem (Mb): 244.8642578125\n",
      "Epoch 5/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.5304785966873169  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.5831008553504944  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.5500917434692383  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.5470550060272217  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.5690917372703552  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.5530188679695129  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.545130729675293  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.5917226672172546  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.5772691369056702  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6218634843826294  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.5139790177345276  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.5273126363754272  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.5538695454597473  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.5168004631996155  AllocMem (Mb): 250.6142578125\n",
      "Epoch 6/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.4960535764694214  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.5562911033630371  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.508242666721344  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.514928936958313  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.5356363654136658  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.5272687673568726  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.531376302242279  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.6453490853309631  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.5783154368400574  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.6417959332466125  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.49403101205825806  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.511447012424469  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.5486055612564087  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.5011021494865417  AllocMem (Mb): 250.6142578125\n",
      "valid  --- Current step: 0  Loss: 0.5471118688583374  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 10  Loss: 0.5084514021873474  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 20  Loss: 0.519603431224823  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 30  Loss: 0.3908345699310303  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 40  Loss: 0.5720871686935425  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 50  Loss: 0.5654240250587463  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 60  Loss: 0.5270336270332336  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 70  Loss: 0.37466198205947876  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 80  Loss: 0.5415551066398621  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 90  Loss: 0.5561923384666443  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 100  Loss: 0.560472846031189  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 110  Loss: 0.5688179135322571  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 120  Loss: 0.5718396306037903  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 130  Loss: 0.6636250019073486  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 140  Loss: 0.5149228572845459  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 150  Loss: 0.4943509101867676  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 160  Loss: 0.44354283809661865  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 170  Loss: 0.5191249847412109  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 180  Loss: 0.6473128795623779  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 190  Loss: 0.48288971185684204  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 200  Loss: 0.4090518057346344  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 210  Loss: 0.5549998879432678  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 220  Loss: 0.5279145240783691  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 230  Loss: 0.5075282454490662  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 240  Loss: 0.5275704264640808  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 250  Loss: 0.5948253870010376  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 260  Loss: 0.530428946018219  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 270  Loss: 0.5838550329208374  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 280  Loss: 0.4950515329837799  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 290  Loss: 0.5260989665985107  AllocMem (Mb): 244.8642578125\n",
      "Epoch 7/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.4731050133705139  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.5487227439880371  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.5181440711021423  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.5004938840866089  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.522753894329071  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.5121163129806519  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.5010991096496582  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.5732004642486572  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.5344704985618591  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.5981119275093079  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.4486795961856842  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.4629012644290924  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.4851079285144806  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.44524675607681274  AllocMem (Mb): 250.6142578125\n",
      "Epoch 8/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.44278886914253235  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.5079453587532043  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.4691441059112549  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.46303996443748474  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.4803122878074646  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.4630664587020874  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.47053417563438416  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.541858971118927  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.48398926854133606  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 90  Loss: 0.5810371041297913  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 100  Loss: 0.42679300904273987  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 110  Loss: 0.40934330224990845  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 120  Loss: 0.4262841045856476  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 130  Loss: 0.37838926911354065  AllocMem (Mb): 250.6142578125\n",
      "valid  --- Current step: 0  Loss: 0.45973098278045654  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 10  Loss: 0.34924381971359253  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 20  Loss: 0.4199342131614685  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 30  Loss: 0.21943703293800354  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 40  Loss: 0.4842584729194641  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 50  Loss: 0.47730326652526855  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 60  Loss: 0.39138779044151306  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 70  Loss: 0.19965830445289612  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 80  Loss: 0.4109076261520386  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 90  Loss: 0.49314770102500916  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 100  Loss: 0.45826753973960876  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 110  Loss: 0.5267327427864075  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 120  Loss: 0.4800132215023041  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 130  Loss: 0.6395266056060791  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 140  Loss: 0.40728992223739624  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 150  Loss: 0.36526820063591003  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 160  Loss: 0.2967282235622406  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 170  Loss: 0.39124342799186707  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 180  Loss: 0.8217489719390869  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 190  Loss: 0.36304330825805664  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 200  Loss: 0.26674801111221313  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 210  Loss: 0.46715375781059265  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 220  Loss: 0.5288762450218201  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 230  Loss: 0.38426700234413147  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 240  Loss: 0.42709407210350037  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 250  Loss: 0.5306606292724609  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 260  Loss: 0.4342201352119446  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 270  Loss: 0.5252167582511902  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 280  Loss: 0.36103010177612305  AllocMem (Mb): 244.8642578125\n",
      "valid  --- Current step: 290  Loss: 0.3965603709220886  AllocMem (Mb): 244.8642578125\n",
      "Epoch 9/99\n",
      "----------\n",
      "train  --- Current step: 0  Loss: 0.4113052487373352  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 10  Loss: 0.471194863319397  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 20  Loss: 0.44244715571403503  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 30  Loss: 0.4243929088115692  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 40  Loss: 0.43842455744743347  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 50  Loss: 0.40680772066116333  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 60  Loss: 0.4508287310600281  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 70  Loss: 0.4561353027820587  AllocMem (Mb): 250.6142578125\n",
      "train  --- Current step: 80  Loss: 0.42405155301094055  AllocMem (Mb): 250.6142578125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-112c67771fda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mtrain_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mtrain_Adam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mtrain_SCRN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSEED\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-51839417297a>\u001b[0m in \u001b[0;36mtrain_SGD\u001b[0;34m(train_dl, valid_dl, w, LR_BASE, savepath, SEED)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     model = train(unet, device, train_dl, valid_dl, loss_fn, opt, epochs=100,\n\u001b[0;32m---> 60\u001b[0;31m                   lr_base = LR_BASE, save=True, savepath=savepath_)\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#empty gpu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9cab60e1616e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_dl, valid_dl, loss_fn, optimizer, epochs, lr_base, save, savepath, mode)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                             \u001b[0my_hat_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# format the outputs to be numpy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                             \u001b[0mtrain_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9cab60e1616e>\u001b[0m in \u001b[0;36mget_outputs\u001b[0;34m(y_hat, val)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m   \u001b[0myh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_input_, train_output_ = load_data()\n",
    "SEEDS = [0,1,2,3,4]\n",
    "\n",
    "\n",
    "for SEED in SEEDS:\n",
    "\n",
    "  set_seed(SEED) # seed for model init\n",
    "\n",
    "  train_input, train_output, val_input, val_output = split_train_val(train_input_,SEED=SEED) # also change the train / val split\n",
    "  train_dl, valid_dl, w = get_dataloaders(train_input, train_output, val_input, val_output, BATCH_SIZE = 5)\n",
    "\n",
    "  train_SGD(train_dl, valid_dl, w, SEED=SEED)\n",
    "  train_Adam(train_dl, valid_dl, w, SEED=SEED)\n",
    "  train_SCRN(train_dl, valid_dl, w, SEED=SEED)\n",
    "  train_AdaHessian(train_dl, valid_dl, w, SEED=SEED)\n",
    "  \n",
    "            \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Training a DNN for HC18 segmentation task.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
