{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell below is need if we want to run this notebook on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGF43y9-Oz0D",
    "outputId": "7ddfb9f6-7ab7-4d9a-fd8b-0e0e7d06cf2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# code required for running this notebook on colab. \n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/ML-optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-ZlI-wxq0vE4"
   },
   "outputs": [],
   "source": [
    "# importing the needed packages for this experiment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "import nfft\n",
    "from sklearn.decomposition import PCA\n",
    "from pytorch_optimizer import load_optimizer\n",
    "from determinestic import set_seed\n",
    "from custome_dataset import CustomMnist\n",
    "from SCRNOptimizer import SCRNOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "In this part we set seeds (along with other commands) to make the code reproducible. Refer to `deterministic.py` for more info. We try with 5 random seeds in this experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fl2j6v2uls3Z",
    "outputId": "9886fd39-48d7-4dcf-d070-ec480f125290"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/ML-optimisation/determinestic.py:37: UserWarning: Enabeling deterministic mode in PyTorch can have a performance impact when using GPU.\n",
      "  'Enabeling deterministic mode in PyTorch can have a performance '\n"
     ]
    }
   ],
   "source": [
    "set_seed(1)\n",
    "# set_seed(44)\n",
    "# set_seed(12)\n",
    "# set_seed(15)\n",
    "# set_seed(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Von-vmK4Lzj4"
   },
   "source": [
    "# Preparing Data\n",
    "\n",
    "The `MNIST` object is a subclass of a PyTorch [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) class, which as we'll see later can be used with a `DataLoader` object to provide streaming access to batches of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuK8t9RFME8v",
    "outputId": "c12ff21a-1b45-4b4b-c2a7-fda8b6fa9217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CustomMnist\n",
      "    Number of datapoints: 60000\n",
      "    Root location: .\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "# load (download if needed) the MNIST dataset\n",
    "mnist_train = CustomMnist(\".\", train=True, download=True,)\n",
    "mnist_test = CustomMnist(\".\", train=False, download=True,)\n",
    "print (mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "43nIuVEJdUQV",
    "outputId": "50ea3af4-cb45-4e8c-e07f-c92958841b21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 784]), torch.Size([10000, 784]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flattening the data as we use a two layer MLP to learn the data\n",
    "flat_mnist_train = mnist_train.data.reshape(mnist_train.data.shape[0],mnist_train.data.shape[1]*mnist_train.data.shape[2])\n",
    "flat_mnist_test = mnist_test.data.reshape(mnist_test.data.shape[0],mnist_test.data.shape[1]*mnist_test.data.shape[2])\n",
    "flat_mnist_train.shape, flat_mnist_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doing the dimensionality reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U9_AMrWpb5Lm",
    "outputId": "1a26e2ee-2a18-4711-b893-1d92213b2524"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PCA(n_components=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get PCA in this cell to be able to do non uniform fast Fourier transform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training set only.\n",
    "scaler.fit(flat_mnist_train)\n",
    "\n",
    "# Apply transform to both the training set and the test set.\n",
    "mnist_train.data = scaler.transform(flat_mnist_train)\n",
    "mnist_test.data = scaler.transform(flat_mnist_test)\n",
    "\n",
    "print(mnist_train.data.shape, mnist_test.data.shape)\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(mnist_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mITiJqz3hUQj",
    "outputId": "7e37fe1b-a820-4e65-bdc7-fbcae35cc055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(48000, 12000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we split the train dataset to validation and train sets\n",
    "total_count=len(mnist_train)\n",
    "print(total_count)\n",
    "train_count = int(0.80 * total_count)\n",
    "val_count = total_count - train_count\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(\n",
    "    mnist_train, (train_count,val_count)\n",
    ")\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "A_geUUxuU49x"
   },
   "outputs": [],
   "source": [
    "# create data loaders\n",
    "trainloader = DataLoader(train_set, batch_size=128, shuffle=True)\n",
    "valloader = DataLoader(val_set, batch_size=128, shuffle=True)\n",
    "testloader = DataLoader(mnist_test, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LC_mJsByVKFl"
   },
   "outputs": [],
   "source": [
    "# define baseline model which is a two layer fully connected network with ReLU activation.\n",
    "class BaselineModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        # self.bn= nn.BatchNorm1d(64),\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "eQpq5G4ojNc-",
    "outputId": "450123d1-4d16-4b78-9705-87ab97a90718"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting the device for execution\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining functions to train and evaluate the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "P6jCy8BNsAl_"
   },
   "outputs": [],
   "source": [
    "# define the to train models\n",
    "def train(model, data_loader, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    x_pca=[]\n",
    "    y_hat=[]\n",
    "    y_true=[]\n",
    "    correct=0\n",
    "    # main training loop for one epoch\n",
    "    for batch, tensor in enumerate(data_loader):\n",
    "        data, target = tensor\n",
    "        # doing the PCA analysis\n",
    "        x_pca+=np.squeeze(pca.transform(data)).tolist()\n",
    "        y_true+=target.tolist()\n",
    "        data=data.to(device)\n",
    "        target=target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # forward + loss + backward + optimise (update weights)\n",
    "        out = model(data)\n",
    "        predicted = torch.argmax(out, dim=1)\n",
    "        y_hat += predicted.tolist()\n",
    "        correct += torch.sum(target==predicted).item()\n",
    "\n",
    "        loss = loss_function(out,target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward(create_graph=True, retain_graph=True)\n",
    "        optimizer.step()\n",
    "    x_pca=torch.tensor(x_pca)\n",
    "    y_true=torch.tensor(y_true)\n",
    "    y_hat=torch.tensor(y_hat)\n",
    "    \n",
    "    # print(x_pca.shape)\n",
    "    # Reporting the average loss, average accuracy and the error of Fourier coefficients as explained in the report.\n",
    "    avg_accuracy = correct / (1*len(data_loader.dataset))\n",
    "    avg_loss = train_loss / len(data_loader.dataset)\n",
    "    nfft_out_hat = nfft.nfft_adjoint(x_pca, y_hat , 20)\n",
    "    nfft_out_true = nfft.nfft_adjoint(x_pca, y_true, 20)\n",
    "    avg_nfft = np.abs((nfft_out_hat-nfft_out_true)/nfft_out_true)\n",
    "    return avg_loss,avg_accuracy, avg_nfft\n",
    "           \n",
    "# define the to evaluate models            \n",
    "def test(model, data_loader,):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    nfft_out=0\n",
    "    x_pca=[]\n",
    "    y_hat=[]\n",
    "    y_true=[]\n",
    "    with torch.no_grad():\n",
    "        for batch, tensor in enumerate(data_loader):\n",
    "            data, target = tensor\n",
    "            x_pca+=np.squeeze(pca.transform(data)).tolist()\n",
    "            y_true+=target.tolist()\n",
    "            data=data.to(device)\n",
    "            target=target.to(device)\n",
    "            # forward + loss + backward + optimise (update weights)\n",
    "            out = model(data)\n",
    "            test_loss += loss_function(out, target).item()\n",
    "            predicted = torch.argmax(out, dim=1)\n",
    "            y_hat+=predicted.tolist()\n",
    "            correct += torch.sum(target==predicted).item()\n",
    "\n",
    "    x_pca=torch.tensor(x_pca)\n",
    "    y_true=torch.tensor(y_true)\n",
    "    y_hat=torch.tensor(y_hat) \n",
    "\n",
    "    # Reporting the average loss, average accuracy and the error of Fourier coefficients as explained in the report.\n",
    "    avg_accuracy = correct / (1*len(data_loader.dataset))\n",
    "    avg_loss = test_loss / (1*len(data_loader.dataset))\n",
    "    nfft_out_hat = nfft.nfft_adjoint(x_pca, y_hat , 20)\n",
    "    nfft_out_true = nfft.nfft_adjoint(x_pca, y_true , 20)\n",
    "    avg_nfft = np.abs((nfft_out_hat-nfft_out_true)/nfft_out_true)\n",
    "\n",
    "    return avg_loss, avg_accuracy, avg_nfft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model \n",
    "model = BaselineModel(784, 784, 10).to(device)\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "# learning_momentum = 0.9\n",
    "optimiser = optim.SGD(model.parameters(), lr=learning_rate,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LWSsTySug_kb",
    "outputId": "c999b472-91c1-4961-fa69-be51ae2e5059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:175: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at  ../torch/csrc/autograd/engine.cpp:985.)\n",
      "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss= 0.0080, val loss= 0.0041, val_Accuracy=87.4917%, test loss= 0.0041, test_Accuracy=87.8300%\n",
      "Epoch 2: Training loss= 0.0034, val loss= 0.0029, val_Accuracy=90.4250%, test loss= 0.0028, test_Accuracy=90.3700%\n",
      "Epoch 3: Training loss= 0.0026, val loss= 0.0024, val_Accuracy=91.5167%, test loss= 0.0024, test_Accuracy=91.5400%\n",
      "Epoch 4: Training loss= 0.0023, val loss= 0.0022, val_Accuracy=92.1750%, test loss= 0.0021, test_Accuracy=92.2200%\n",
      "Epoch 5: Training loss= 0.0020, val loss= 0.0020, val_Accuracy=92.6917%, test loss= 0.0020, test_Accuracy=92.7400%\n",
      "Epoch 6: Training loss= 0.0019, val loss= 0.0019, val_Accuracy=93.0917%, test loss= 0.0018, test_Accuracy=93.2200%\n",
      "Epoch 7: Training loss= 0.0017, val loss= 0.0018, val_Accuracy=93.4000%, test loss= 0.0017, test_Accuracy=93.5200%\n",
      "Epoch 8: Training loss= 0.0016, val loss= 0.0017, val_Accuracy=93.7167%, test loss= 0.0016, test_Accuracy=93.8600%\n",
      "Epoch 9: Training loss= 0.0015, val loss= 0.0016, val_Accuracy=93.9583%, test loss= 0.0016, test_Accuracy=94.0300%\n",
      "Epoch 10: Training loss= 0.0015, val loss= 0.0016, val_Accuracy=94.1333%, test loss= 0.0015, test_Accuracy=94.3400%\n",
      "Epoch 11: Training loss= 0.0014, val loss= 0.0015, val_Accuracy=94.2917%, test loss= 0.0015, test_Accuracy=94.5000%\n",
      "Epoch 12: Training loss= 0.0013, val loss= 0.0015, val_Accuracy=94.5000%, test loss= 0.0015, test_Accuracy=94.7300%\n",
      "Epoch 13: Training loss= 0.0013, val loss= 0.0014, val_Accuracy=94.6417%, test loss= 0.0014, test_Accuracy=94.8500%\n",
      "Epoch 14: Training loss= 0.0012, val loss= 0.0014, val_Accuracy=94.8583%, test loss= 0.0014, test_Accuracy=95.0400%\n",
      "Epoch 15: Training loss= 0.0012, val loss= 0.0014, val_Accuracy=94.9667%, test loss= 0.0013, test_Accuracy=95.1800%\n",
      "Epoch 16: Training loss= 0.0011, val loss= 0.0013, val_Accuracy=95.0750%, test loss= 0.0013, test_Accuracy=95.3400%\n",
      "Epoch 17: Training loss= 0.0011, val loss= 0.0013, val_Accuracy=95.2750%, test loss= 0.0012, test_Accuracy=95.4500%\n",
      "Epoch 18: Training loss= 0.0010, val loss= 0.0013, val_Accuracy=95.3750%, test loss= 0.0012, test_Accuracy=95.5700%\n",
      "Epoch 19: Training loss= 0.0010, val loss= 0.0012, val_Accuracy=95.5417%, test loss= 0.0012, test_Accuracy=95.6600%\n",
      "Epoch 20: Training loss= 0.0010, val loss= 0.0012, val_Accuracy=95.5917%, test loss= 0.0012, test_Accuracy=95.7300%\n",
      "Epoch 21: Training loss= 0.0009, val loss= 0.0012, val_Accuracy=95.6333%, test loss= 0.0011, test_Accuracy=95.8000%\n",
      "Epoch 22: Training loss= 0.0009, val loss= 0.0012, val_Accuracy=95.6917%, test loss= 0.0011, test_Accuracy=95.9000%\n",
      "Epoch 23: Training loss= 0.0009, val loss= 0.0012, val_Accuracy=95.7750%, test loss= 0.0011, test_Accuracy=96.0200%\n",
      "Epoch 24: Training loss= 0.0009, val loss= 0.0011, val_Accuracy=95.8500%, test loss= 0.0011, test_Accuracy=96.1000%\n",
      "Epoch 25: Training loss= 0.0008, val loss= 0.0011, val_Accuracy=95.8750%, test loss= 0.0011, test_Accuracy=96.1500%\n",
      "Epoch 26: Training loss= 0.0008, val loss= 0.0011, val_Accuracy=95.9417%, test loss= 0.0010, test_Accuracy=96.2200%\n",
      "Epoch 27: Training loss= 0.0008, val loss= 0.0011, val_Accuracy=96.0333%, test loss= 0.0010, test_Accuracy=96.2800%\n",
      "Epoch 28: Training loss= 0.0008, val loss= 0.0011, val_Accuracy=96.0417%, test loss= 0.0010, test_Accuracy=96.3100%\n",
      "Epoch 29: Training loss= 0.0007, val loss= 0.0011, val_Accuracy=96.0833%, test loss= 0.0010, test_Accuracy=96.3600%\n",
      "Epoch 30: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.1750%, test loss= 0.0010, test_Accuracy=96.5200%\n",
      "Epoch 31: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.2083%, test loss= 0.0010, test_Accuracy=96.5000%\n",
      "Epoch 32: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.2000%, test loss= 0.0010, test_Accuracy=96.5400%\n",
      "Epoch 33: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.2750%, test loss= 0.0010, test_Accuracy=96.6200%\n",
      "Epoch 34: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.3167%, test loss= 0.0010, test_Accuracy=96.6700%\n",
      "Epoch 35: Training loss= 0.0006, val loss= 0.0010, val_Accuracy=96.3750%, test loss= 0.0009, test_Accuracy=96.6800%\n",
      "Epoch 36: Training loss= 0.0006, val loss= 0.0010, val_Accuracy=96.3667%, test loss= 0.0009, test_Accuracy=96.7300%\n",
      "Epoch 37: Training loss= 0.0006, val loss= 0.0010, val_Accuracy=96.4667%, test loss= 0.0009, test_Accuracy=96.7500%\n",
      "Epoch 38: Training loss= 0.0006, val loss= 0.0010, val_Accuracy=96.4500%, test loss= 0.0009, test_Accuracy=96.7800%\n",
      "Epoch 39: Training loss= 0.0006, val loss= 0.0009, val_Accuracy=96.5083%, test loss= 0.0009, test_Accuracy=96.8200%\n",
      "Epoch 40: Training loss= 0.0006, val loss= 0.0009, val_Accuracy=96.5583%, test loss= 0.0009, test_Accuracy=96.8400%\n",
      "Epoch 41: Training loss= 0.0006, val loss= 0.0009, val_Accuracy=96.6250%, test loss= 0.0009, test_Accuracy=96.9100%\n",
      "Epoch 42: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.6167%, test loss= 0.0009, test_Accuracy=96.8600%\n",
      "Epoch 43: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.6500%, test loss= 0.0009, test_Accuracy=96.8900%\n",
      "Epoch 44: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.7167%, test loss= 0.0009, test_Accuracy=96.9500%\n",
      "Epoch 45: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.6250%, test loss= 0.0009, test_Accuracy=96.9500%\n",
      "Epoch 46: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.7583%, test loss= 0.0009, test_Accuracy=96.9400%\n",
      "Epoch 47: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.7833%, test loss= 0.0009, test_Accuracy=96.9700%\n",
      "Epoch 48: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.7833%, test loss= 0.0008, test_Accuracy=96.9900%\n",
      "Epoch 49: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.8750%, test loss= 0.0008, test_Accuracy=97.0100%\n",
      "Epoch 50: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.8667%, test loss= 0.0008, test_Accuracy=97.0200%\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "SGD_epoch_nums = []\n",
    "SGD_training_loss = []\n",
    "SGD_training_acc = []\n",
    "SGD_training_nfft = []\n",
    "SGD_validation_loss = []\n",
    "SGD_validation_acc = []\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss,train_accuracy, train_nfft = train(model, trainloader, optimiser,)\n",
    "    # print(train_nfft)\n",
    "    val_loss, val_accuracy, val_nfft = test(model, valloader,)\n",
    "    test_loss, test_accuracy, test_nfft = test(model, testloader,)\n",
    "    SGD_epoch_nums.append(epoch)\n",
    "    SGD_training_loss.append(train_loss)\n",
    "    SGD_training_nfft.append(train_nfft)\n",
    "    SGD_validation_loss.append(val_loss)\n",
    "    SGD_validation_acc.append(val_accuracy)\n",
    "    SGD_training_acc.append(train_accuracy)\n",
    "    \n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "    print('Epoch {:d}: Training loss= {:.4f}, val loss= {:.4f}, val_Accuracy={:.4%}, test loss= {:.4f}, test_Accuracy={:.4%}'.format(epoch, train_loss,\n",
    "                                                                                            val_loss, val_accuracy,test_loss, test_accuracy,))\n",
    "    \n",
    "\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOSfCmCYuApz",
    "outputId": "70e7bd82-3431-44ed-fab0-2b16b20209c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "np.save(\"./SGDdata1\",np.array([SGD_epoch_nums,SGD_training_loss, SGD_training_acc,SGD_training_nfft,\n",
    "                                                                   SGD_validation_loss,SGD_validation_acc]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fbh_Ut1qg_5D"
   },
   "outputs": [],
   "source": [
    "# build the model \n",
    "model = BaselineModel(784, 784, 10).to(device)\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimiser = optim.Adam(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qratFqUg_5D",
    "outputId": "87957e9b-a6c0-4332-b3d0-faf14259944a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss= 0.0455, val loss= 0.0510, val_Accuracy=92.6750%, test loss= 0.0331, test_Accuracy=93.0500%\n",
      "Epoch 2: Training loss= 0.0390, val loss= 0.0968, val_Accuracy=94.4167%, test loss= 0.0477, test_Accuracy=94.4600%\n",
      "Epoch 3: Training loss= 0.0280, val loss= 0.0695, val_Accuracy=95.8250%, test loss= 0.0420, test_Accuracy=95.6700%\n",
      "Epoch 4: Training loss= 0.0127, val loss= 0.0964, val_Accuracy=95.0083%, test loss= 0.0432, test_Accuracy=95.2800%\n",
      "Epoch 5: Training loss= 0.0162, val loss= 0.0649, val_Accuracy=95.3083%, test loss= 0.0424, test_Accuracy=96.2200%\n",
      "Epoch 6: Training loss= 0.0151, val loss= 0.0576, val_Accuracy=96.3417%, test loss= 0.0583, test_Accuracy=96.4000%\n",
      "Epoch 7: Training loss= 0.0082, val loss= 0.0557, val_Accuracy=96.5833%, test loss= 0.0500, test_Accuracy=96.5900%\n",
      "Epoch 8: Training loss= 0.0057, val loss= 0.0670, val_Accuracy=96.3750%, test loss= 0.0523, test_Accuracy=96.4000%\n",
      "Epoch 9: Training loss= 0.0063, val loss= 0.0663, val_Accuracy=96.5500%, test loss= 0.0536, test_Accuracy=96.6300%\n",
      "Epoch 10: Training loss= 0.0063, val loss= 0.0544, val_Accuracy=96.7667%, test loss= 0.0521, test_Accuracy=96.8600%\n",
      "Epoch 11: Training loss= 0.0069, val loss= 0.0630, val_Accuracy=96.7667%, test loss= 0.0585, test_Accuracy=96.8300%\n",
      "Epoch 12: Training loss= 0.0087, val loss= 0.0588, val_Accuracy=96.3833%, test loss= 0.0674, test_Accuracy=96.7100%\n",
      "Epoch 13: Training loss= 0.0076, val loss= 0.0722, val_Accuracy=96.6250%, test loss= 0.0723, test_Accuracy=96.6800%\n",
      "Epoch 14: Training loss= 0.0083, val loss= 0.0755, val_Accuracy=96.8750%, test loss= 0.0772, test_Accuracy=96.8300%\n",
      "Epoch 15: Training loss= 0.0087, val loss= 0.0660, val_Accuracy=96.8833%, test loss= 0.0716, test_Accuracy=97.0000%\n",
      "Epoch 16: Training loss= 0.0094, val loss= 0.0678, val_Accuracy=96.6167%, test loss= 0.0899, test_Accuracy=96.3300%\n",
      "Epoch 17: Training loss= 0.0084, val loss= 0.0634, val_Accuracy=96.8250%, test loss= 0.0911, test_Accuracy=96.8800%\n",
      "Epoch 18: Training loss= 0.0133, val loss= 0.2780, val_Accuracy=96.1917%, test loss= 0.1284, test_Accuracy=96.2300%\n",
      "Epoch 19: Training loss= 0.0172, val loss= 0.2652, val_Accuracy=96.5417%, test loss= 0.1361, test_Accuracy=96.6000%\n",
      "Epoch 20: Training loss= 0.0060, val loss= 0.2197, val_Accuracy=97.3000%, test loss= 0.1202, test_Accuracy=97.4000%\n",
      "Epoch 21: Training loss= 0.0061, val loss= 0.1766, val_Accuracy=97.1167%, test loss= 0.1160, test_Accuracy=97.1400%\n",
      "Epoch 22: Training loss= 0.0034, val loss= 0.1867, val_Accuracy=97.0917%, test loss= 0.1245, test_Accuracy=97.1100%\n",
      "Epoch 23: Training loss= 0.0042, val loss= 0.1653, val_Accuracy=97.0417%, test loss= 0.1241, test_Accuracy=97.0000%\n",
      "Epoch 24: Training loss= 0.0065, val loss= 0.2477, val_Accuracy=97.3750%, test loss= 0.1384, test_Accuracy=97.2600%\n",
      "Epoch 25: Training loss= 0.0055, val loss= 0.1453, val_Accuracy=96.9167%, test loss= 0.1844, test_Accuracy=96.8600%\n",
      "Epoch 26: Training loss= 0.0160, val loss= 0.2509, val_Accuracy=96.6167%, test loss= 0.1893, test_Accuracy=96.4400%\n",
      "Epoch 27: Training loss= 0.0107, val loss= 0.1371, val_Accuracy=97.2083%, test loss= 0.1831, test_Accuracy=97.1400%\n",
      "Epoch 28: Training loss= 0.0109, val loss= 0.2103, val_Accuracy=96.9250%, test loss= 0.2317, test_Accuracy=96.9900%\n",
      "Epoch 29: Training loss= 0.0118, val loss= 0.1372, val_Accuracy=97.2750%, test loss= 0.2016, test_Accuracy=97.3400%\n",
      "Epoch 30: Training loss= 0.0063, val loss= 0.2192, val_Accuracy=97.1917%, test loss= 0.1943, test_Accuracy=97.2300%\n",
      "Epoch 31: Training loss= 0.0037, val loss= 0.2771, val_Accuracy=97.5000%, test loss= 0.1981, test_Accuracy=97.5600%\n",
      "Epoch 32: Training loss= 0.0024, val loss= 0.1233, val_Accuracy=97.3583%, test loss= 0.1991, test_Accuracy=97.4400%\n",
      "Epoch 33: Training loss= 0.0028, val loss= 0.1294, val_Accuracy=97.4250%, test loss= 0.2011, test_Accuracy=97.3700%\n",
      "Epoch 34: Training loss= 0.0039, val loss= 0.2299, val_Accuracy=97.4833%, test loss= 0.2129, test_Accuracy=97.5700%\n",
      "Epoch 35: Training loss= 0.0219, val loss= 0.6975, val_Accuracy=96.7667%, test loss= 0.4824, test_Accuracy=96.7100%\n",
      "Epoch 36: Training loss= 0.0247, val loss= 1.6329, val_Accuracy=97.0750%, test loss= 0.5257, test_Accuracy=97.1700%\n",
      "Epoch 37: Training loss= 0.0144, val loss= 0.8274, val_Accuracy=97.3750%, test loss= 0.4254, test_Accuracy=97.5100%\n",
      "Epoch 38: Training loss= 0.0113, val loss= 0.5982, val_Accuracy=97.5917%, test loss= 0.4563, test_Accuracy=97.6100%\n",
      "Epoch 39: Training loss= 0.0015, val loss= 0.5204, val_Accuracy=97.3833%, test loss= 0.4447, test_Accuracy=97.4400%\n",
      "Epoch 40: Training loss= 0.0014, val loss= 0.5753, val_Accuracy=97.5000%, test loss= 0.4486, test_Accuracy=97.3500%\n",
      "Epoch 41: Training loss= 0.0018, val loss= 0.5668, val_Accuracy=97.4167%, test loss= 0.4274, test_Accuracy=97.5400%\n",
      "Epoch 42: Training loss= 0.0020, val loss= 0.5420, val_Accuracy=97.3583%, test loss= 0.4384, test_Accuracy=97.3100%\n",
      "Epoch 43: Training loss= 0.0029, val loss= 0.5929, val_Accuracy=97.3500%, test loss= 0.4555, test_Accuracy=97.3800%\n",
      "Epoch 44: Training loss= 0.0313, val loss= 1.3234, val_Accuracy=96.5000%, test loss= 0.7521, test_Accuracy=96.4400%\n",
      "Epoch 45: Training loss= 0.0714, val loss= 1.0147, val_Accuracy=97.1000%, test loss= 0.8138, test_Accuracy=96.6800%\n",
      "Epoch 46: Training loss= 0.0305, val loss= 1.9004, val_Accuracy=96.6417%, test loss= 0.8683, test_Accuracy=96.7400%\n",
      "Epoch 47: Training loss= 0.0237, val loss= 1.2407, val_Accuracy=97.4833%, test loss= 0.6449, test_Accuracy=97.5100%\n",
      "Epoch 48: Training loss= 0.0010, val loss= 1.2320, val_Accuracy=97.6417%, test loss= 0.6398, test_Accuracy=97.4000%\n",
      "Epoch 49: Training loss= 0.0005, val loss= 1.2580, val_Accuracy=97.7250%, test loss= 1.9384, test_Accuracy=97.4300%\n",
      "Epoch 50: Training loss= 0.0006, val loss= 1.2343, val_Accuracy=97.5500%, test loss= 0.6529, test_Accuracy=97.4100%\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "Adam_epoch_nums = []\n",
    "Adam_training_loss = []\n",
    "Adam_training_acc = []\n",
    "Adam_training_nfft = []\n",
    "Adam_validation_loss = []\n",
    "Adam_validation_acc = []\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss,train_accuracy, train_nfft = train(model, trainloader, optimiser,)\n",
    "    # print(train_nfft)\n",
    "    val_loss, val_accuracy, val_nfft = test(model, valloader,)\n",
    "    test_loss, test_accuracy, test_nfft = test(model, testloader,)\n",
    "    Adam_epoch_nums.append(epoch)\n",
    "    Adam_training_loss.append(train_loss)\n",
    "    Adam_training_nfft.append(train_nfft)\n",
    "    Adam_validation_loss.append(val_loss)\n",
    "    Adam_validation_acc.append(val_accuracy)\n",
    "    Adam_training_acc.append(train_accuracy)\n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "    print('Epoch {:d}: Training loss= {:.4f}, val loss= {:.4f}, val_Accuracy={:.4%}, test loss= {:.4f}, test_Accuracy={:.4%}'.format(epoch, train_loss,\n",
    "                                                                                            val_loss, val_accuracy,test_loss, test_accuracy,))\n",
    "    \n",
    "\n",
    "print('**** Finished Training ****')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UPXFr8XODRvz",
    "outputId": "a55cb2cc-7887-47dc-84b6-995aa55a64da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "np.save(\"./Adamdata1\",np.array([Adam_epoch_nums,Adam_training_loss,\n",
    "                                                                   Adam_training_acc,Adam_training_nfft,\n",
    "                                                                   Adam_validation_loss,Adam_validation_acc]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for AdaHessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1T0LWjJBdIU2"
   },
   "outputs": [],
   "source": [
    "# build the model \n",
    "model = BaselineModel(784, 784, 10).to(device)\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "learning_rate = 1e-2\n",
    "import torch_optimizer as optimadahessian\n",
    "optimiser = optimadahessian.Adahessian(\n",
    "    model.parameters(), lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpgiX0BPdIU2",
    "outputId": "5e1efe60-7004-475d-d805-95787b557698"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss= 0.0052, val loss= 0.0025, val_Accuracy=91.1750%, test loss= 0.0025, test_Accuracy=91.2900%\n",
      "Epoch 2: Training loss= 0.0021, val loss= 0.0019, val_Accuracy=93.2833%, test loss= 0.0018, test_Accuracy=93.2000%\n",
      "Epoch 3: Training loss= 0.0016, val loss= 0.0016, val_Accuracy=94.0917%, test loss= 0.0015, test_Accuracy=94.2700%\n",
      "Epoch 4: Training loss= 0.0013, val loss= 0.0014, val_Accuracy=94.8083%, test loss= 0.0014, test_Accuracy=94.9100%\n",
      "Epoch 5: Training loss= 0.0012, val loss= 0.0013, val_Accuracy=95.2833%, test loss= 0.0012, test_Accuracy=95.4000%\n",
      "Epoch 6: Training loss= 0.0010, val loss= 0.0012, val_Accuracy=95.6083%, test loss= 0.0011, test_Accuracy=95.7800%\n",
      "Epoch 7: Training loss= 0.0009, val loss= 0.0011, val_Accuracy=95.9583%, test loss= 0.0010, test_Accuracy=96.1400%\n",
      "Epoch 8: Training loss= 0.0008, val loss= 0.0010, val_Accuracy=96.2583%, test loss= 0.0010, test_Accuracy=96.3300%\n",
      "Epoch 9: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.3833%, test loss= 0.0009, test_Accuracy=96.6000%\n",
      "Epoch 10: Training loss= 0.0006, val loss= 0.0009, val_Accuracy=96.5750%, test loss= 0.0009, test_Accuracy=96.7900%\n",
      "Epoch 11: Training loss= 0.0006, val loss= 0.0009, val_Accuracy=96.7083%, test loss= 0.0008, test_Accuracy=96.8600%\n",
      "Epoch 12: Training loss= 0.0005, val loss= 0.0008, val_Accuracy=96.8500%, test loss= 0.0008, test_Accuracy=97.0600%\n",
      "Epoch 13: Training loss= 0.0005, val loss= 0.0008, val_Accuracy=96.9500%, test loss= 0.0008, test_Accuracy=97.0400%\n",
      "Epoch 14: Training loss= 0.0004, val loss= 0.0008, val_Accuracy=96.9750%, test loss= 0.0008, test_Accuracy=97.1900%\n",
      "Epoch 15: Training loss= 0.0004, val loss= 0.0008, val_Accuracy=97.1750%, test loss= 0.0007, test_Accuracy=97.2100%\n",
      "Epoch 16: Training loss= 0.0004, val loss= 0.0008, val_Accuracy=97.1917%, test loss= 0.0007, test_Accuracy=97.3200%\n",
      "Epoch 17: Training loss= 0.0003, val loss= 0.0007, val_Accuracy=97.2083%, test loss= 0.0007, test_Accuracy=97.3500%\n",
      "Epoch 18: Training loss= 0.0003, val loss= 0.0007, val_Accuracy=97.2583%, test loss= 0.0007, test_Accuracy=97.4100%\n",
      "Epoch 19: Training loss= 0.0003, val loss= 0.0007, val_Accuracy=97.3167%, test loss= 0.0007, test_Accuracy=97.4800%\n",
      "Epoch 20: Training loss= 0.0003, val loss= 0.0007, val_Accuracy=97.3333%, test loss= 0.0007, test_Accuracy=97.5200%\n",
      "Epoch 21: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.4333%, test loss= 0.0007, test_Accuracy=97.5200%\n",
      "Epoch 22: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.3833%, test loss= 0.0007, test_Accuracy=97.6200%\n",
      "Epoch 23: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.4500%, test loss= 0.0007, test_Accuracy=97.5300%\n",
      "Epoch 24: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.4417%, test loss= 0.0007, test_Accuracy=97.6500%\n",
      "Epoch 25: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.4583%, test loss= 0.0007, test_Accuracy=97.6600%\n",
      "Epoch 26: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.4167%, test loss= 0.0007, test_Accuracy=97.6000%\n",
      "Epoch 27: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5000%, test loss= 0.0007, test_Accuracy=97.6300%\n",
      "Epoch 28: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5083%, test loss= 0.0007, test_Accuracy=97.6600%\n",
      "Epoch 29: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4583%, test loss= 0.0007, test_Accuracy=97.6700%\n",
      "Epoch 30: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5333%, test loss= 0.0007, test_Accuracy=97.6200%\n",
      "Epoch 31: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5500%, test loss= 0.0007, test_Accuracy=97.6200%\n",
      "Epoch 32: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4917%, test loss= 0.0007, test_Accuracy=97.6400%\n",
      "Epoch 33: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4417%, test loss= 0.0007, test_Accuracy=97.5900%\n",
      "Epoch 34: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5417%, test loss= 0.0007, test_Accuracy=97.6600%\n",
      "Epoch 35: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5250%, test loss= 0.0007, test_Accuracy=97.6300%\n",
      "Epoch 36: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5917%, test loss= 0.0007, test_Accuracy=97.6800%\n",
      "Epoch 37: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5500%, test loss= 0.0007, test_Accuracy=97.7300%\n",
      "Epoch 38: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5167%, test loss= 0.0007, test_Accuracy=97.7000%\n",
      "Epoch 39: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5500%, test loss= 0.0007, test_Accuracy=97.7000%\n",
      "Epoch 40: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5583%, test loss= 0.0007, test_Accuracy=97.7100%\n",
      "Epoch 41: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5583%, test loss= 0.0007, test_Accuracy=97.7300%\n",
      "Epoch 42: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5833%, test loss= 0.0007, test_Accuracy=97.7400%\n",
      "Epoch 43: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.6000%, test loss= 0.0007, test_Accuracy=97.7700%\n",
      "Epoch 44: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.5750%, test loss= 0.0007, test_Accuracy=97.7400%\n",
      "Epoch 45: Training loss= 0.0000, val loss= 0.0007, val_Accuracy=97.5750%, test loss= 0.0007, test_Accuracy=97.8000%\n",
      "Epoch 46: Training loss= 0.0000, val loss= 0.0007, val_Accuracy=97.5417%, test loss= 0.0007, test_Accuracy=97.7500%\n",
      "Epoch 47: Training loss= 0.0000, val loss= 0.0007, val_Accuracy=97.5750%, test loss= 0.0007, test_Accuracy=97.7400%\n",
      "Epoch 48: Training loss= 0.0000, val loss= 0.0007, val_Accuracy=97.6000%, test loss= 0.0007, test_Accuracy=97.7100%\n",
      "Epoch 49: Training loss= 0.0000, val loss= 0.0007, val_Accuracy=97.5500%, test loss= 0.0007, test_Accuracy=97.7300%\n",
      "Epoch 50: Training loss= 0.0000, val loss= 0.0007, val_Accuracy=97.5917%, test loss= 0.0007, test_Accuracy=97.7500%\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "AdaHess_epoch_nums = []\n",
    "AdaHess_training_loss = []\n",
    "AdaHess_training_acc = []\n",
    "AdaHess_training_nfft = []\n",
    "AdaHess_validation_loss = []\n",
    "AdaHess_validation_acc = []\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss,train_accuracy, train_nfft = train(model, trainloader, optimiser,)\n",
    "    # print(train_nfft)\n",
    "    val_loss, val_accuracy, val_nfft = test(model, valloader,)\n",
    "    test_loss, test_accuracy, test_nfft = test(model, testloader,)\n",
    "    AdaHess_epoch_nums.append(epoch)\n",
    "    AdaHess_training_loss.append(train_loss)\n",
    "    AdaHess_training_nfft.append(train_nfft)\n",
    "    AdaHess_validation_loss.append(val_loss)\n",
    "    AdaHess_validation_acc.append(val_accuracy)\n",
    "    AdaHess_training_acc.append(train_accuracy)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "    print('Epoch {:d}: Training loss= {:.4f}, val loss= {:.4f}, val_Accuracy={:.4%}, test loss= {:.4f}, test_Accuracy={:.4%}'.format(epoch, train_loss,\n",
    "                                                                                            val_loss, val_accuracy,test_loss, test_accuracy,))\n",
    "    \n",
    "\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97Hgq3wZvfjQ",
    "outputId": "0747a3b6-faa1-4139-e74b-48694408b1af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "np.save(\"./AdaHessdata1\",np.array([AdaHess_epoch_nums,AdaHess_training_loss,\n",
    "                                                                  AdaHess_training_acc, AdaHess_training_nfft,\n",
    "                                                                   AdaHess_validation_loss, AdaHess_validation_acc]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results for SCRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model \n",
    "model = BaselineModel(784, 784, 10).to(device)\n",
    "\n",
    "# define the loss function and the optimiser\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "optimiser = SCRNOptimizer(model.parameters(), ro=1, l=100, inner_itr=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyuL3Iyv3c4C",
    "outputId": "db6bf7bd-9ee0-41a3-8b40-2488a298167c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Training loss= 0.0037, val loss= 0.0020, val_Accuracy=92.7250%, test loss= 0.0020, test_Accuracy=92.8000%\n",
      "Epoch 2: Training loss= 0.0017, val loss= 0.0016, val_Accuracy=94.1250%, test loss= 0.0015, test_Accuracy=94.1400%\n",
      "Epoch 3: Training loss= 0.0013, val loss= 0.0013, val_Accuracy=94.9833%, test loss= 0.0013, test_Accuracy=95.0700%\n",
      "Epoch 4: Training loss= 0.0011, val loss= 0.0012, val_Accuracy=95.6417%, test loss= 0.0012, test_Accuracy=95.7000%\n",
      "Epoch 5: Training loss= 0.0009, val loss= 0.0011, val_Accuracy=95.8833%, test loss= 0.0011, test_Accuracy=95.9900%\n",
      "Epoch 6: Training loss= 0.0008, val loss= 0.0011, val_Accuracy=96.0417%, test loss= 0.0011, test_Accuracy=96.2200%\n",
      "Epoch 7: Training loss= 0.0007, val loss= 0.0010, val_Accuracy=96.2583%, test loss= 0.0010, test_Accuracy=96.4000%\n",
      "Epoch 8: Training loss= 0.0006, val loss= 0.0010, val_Accuracy=96.4500%, test loss= 0.0009, test_Accuracy=96.6600%\n",
      "Epoch 9: Training loss= 0.0006, val loss= 0.0009, val_Accuracy=96.5667%, test loss= 0.0009, test_Accuracy=96.8500%\n",
      "Epoch 10: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.7500%, test loss= 0.0009, test_Accuracy=96.9300%\n",
      "Epoch 11: Training loss= 0.0005, val loss= 0.0009, val_Accuracy=96.6750%, test loss= 0.0008, test_Accuracy=96.9900%\n",
      "Epoch 12: Training loss= 0.0004, val loss= 0.0008, val_Accuracy=96.8667%, test loss= 0.0008, test_Accuracy=97.1200%\n",
      "Epoch 13: Training loss= 0.0004, val loss= 0.0008, val_Accuracy=96.8750%, test loss= 0.0008, test_Accuracy=97.1000%\n",
      "Epoch 14: Training loss= 0.0004, val loss= 0.0008, val_Accuracy=96.9500%, test loss= 0.0008, test_Accuracy=97.1600%\n",
      "Epoch 15: Training loss= 0.0003, val loss= 0.0008, val_Accuracy=96.9833%, test loss= 0.0008, test_Accuracy=97.1900%\n",
      "Epoch 16: Training loss= 0.0003, val loss= 0.0008, val_Accuracy=97.0500%, test loss= 0.0008, test_Accuracy=97.3400%\n",
      "Epoch 17: Training loss= 0.0003, val loss= 0.0008, val_Accuracy=97.1167%, test loss= 0.0008, test_Accuracy=97.3700%\n",
      "Epoch 18: Training loss= 0.0003, val loss= 0.0008, val_Accuracy=97.0833%, test loss= 0.0008, test_Accuracy=97.4100%\n",
      "Epoch 19: Training loss= 0.0003, val loss= 0.0008, val_Accuracy=97.1583%, test loss= 0.0007, test_Accuracy=97.3900%\n",
      "Epoch 20: Training loss= 0.0002, val loss= 0.0008, val_Accuracy=97.2167%, test loss= 0.0007, test_Accuracy=97.3600%\n",
      "Epoch 21: Training loss= 0.0002, val loss= 0.0008, val_Accuracy=97.1667%, test loss= 0.0007, test_Accuracy=97.3800%\n",
      "Epoch 22: Training loss= 0.0002, val loss= 0.0008, val_Accuracy=97.1583%, test loss= 0.0008, test_Accuracy=97.3300%\n",
      "Epoch 23: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.1917%, test loss= 0.0007, test_Accuracy=97.4100%\n",
      "Epoch 24: Training loss= 0.0002, val loss= 0.0008, val_Accuracy=97.2417%, test loss= 0.0007, test_Accuracy=97.4700%\n",
      "Epoch 25: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.2500%, test loss= 0.0007, test_Accuracy=97.3900%\n",
      "Epoch 26: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.3583%, test loss= 0.0007, test_Accuracy=97.4500%\n",
      "Epoch 27: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.2750%, test loss= 0.0007, test_Accuracy=97.4700%\n",
      "Epoch 28: Training loss= 0.0002, val loss= 0.0007, val_Accuracy=97.3333%, test loss= 0.0007, test_Accuracy=97.4600%\n",
      "Epoch 29: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3833%, test loss= 0.0007, test_Accuracy=97.3900%\n",
      "Epoch 30: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3167%, test loss= 0.0008, test_Accuracy=97.5100%\n",
      "Epoch 31: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3083%, test loss= 0.0007, test_Accuracy=97.5100%\n",
      "Epoch 32: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3833%, test loss= 0.0007, test_Accuracy=97.4600%\n",
      "Epoch 33: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3667%, test loss= 0.0007, test_Accuracy=97.4200%\n",
      "Epoch 34: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3583%, test loss= 0.0007, test_Accuracy=97.4100%\n",
      "Epoch 35: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.2750%, test loss= 0.0007, test_Accuracy=97.3600%\n",
      "Epoch 36: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3667%, test loss= 0.0007, test_Accuracy=97.3700%\n",
      "Epoch 37: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3250%, test loss= 0.0007, test_Accuracy=97.3700%\n",
      "Epoch 38: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3417%, test loss= 0.0007, test_Accuracy=97.3900%\n",
      "Epoch 39: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4167%, test loss= 0.0008, test_Accuracy=97.4200%\n",
      "Epoch 40: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3583%, test loss= 0.0007, test_Accuracy=97.4500%\n",
      "Epoch 41: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3750%, test loss= 0.0007, test_Accuracy=97.3000%\n",
      "Epoch 42: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3667%, test loss= 0.0007, test_Accuracy=97.3500%\n",
      "Epoch 43: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3583%, test loss= 0.0007, test_Accuracy=97.3900%\n",
      "Epoch 44: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3833%, test loss= 0.0008, test_Accuracy=97.4000%\n",
      "Epoch 45: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4000%, test loss= 0.0008, test_Accuracy=97.3900%\n",
      "Epoch 46: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3583%, test loss= 0.0008, test_Accuracy=97.4000%\n",
      "Epoch 47: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4000%, test loss= 0.0008, test_Accuracy=97.4200%\n",
      "Epoch 48: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3417%, test loss= 0.0008, test_Accuracy=97.3700%\n",
      "Epoch 49: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.3583%, test loss= 0.0008, test_Accuracy=97.4100%\n",
      "Epoch 50: Training loss= 0.0001, val loss= 0.0007, val_Accuracy=97.4083%, test loss= 0.0008, test_Accuracy=97.3100%\n",
      "**** Finished Training ****\n"
     ]
    }
   ],
   "source": [
    "SCRN_epoch_nums = []\n",
    "SCRN_training_loss = []\n",
    "SCRN_training_acc = []\n",
    "SCRN_training_nfft = []\n",
    "SCRN_validation_loss = []\n",
    "SCRN_validation_acc = []\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    train_loss,train_accuracy, train_nfft = train(model, trainloader, optimiser,)\n",
    "    # print(train_nfft)\n",
    "    val_loss, val_accuracy, val_nfft = test(model, valloader,)\n",
    "    test_loss, test_accuracy, test_nfft = test(model, testloader,)\n",
    "    SCRN_epoch_nums.append(epoch)\n",
    "    SCRN_training_loss.append(train_loss)\n",
    "    SCRN_training_nfft.append(train_nfft)\n",
    "    SCRN_validation_loss.append(val_loss)\n",
    "    SCRN_validation_acc.append(val_accuracy)\n",
    "    SCRN_training_acc.append(train_accuracy)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # torch.save(model.state_dict(), save_path)\n",
    "    print('Epoch {:d}: Training loss= {:.4f}, val loss= {:.4f}, val_Accuracy={:.4%}, test loss= {:.4f}, test_Accuracy={:.4%}'.format(epoch, train_loss,\n",
    "                                                                                            val_loss, val_accuracy,test_loss, test_accuracy,))\n",
    "    \n",
    "\n",
    "print('**** Finished Training ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./SCRN1\",np.array([SCRN_epoch_nums,SCRN_training_loss,\n",
    "                                                                  SCRN_training_acc, SCRN_training_nfft,\n",
    "                                                                   SCRN_validation_loss, SCRN_validation_acc]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the results\n",
    "This results are based on the averaged results over 5 different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EeiWnZVnPBoo"
   },
   "outputs": [],
   "source": [
    "# here you can find the plot functions \n",
    "\n",
    "import seaborn as sns; sns.set_theme() \n",
    "sns.set_context(\"notebook\",  rc={\"lines.linewidth\": 1}) \n",
    "def plot_Fmap(FP, epoch_tr, ax, cbar=False, title=''): \n",
    " \n",
    "  if cbar: \n",
    "    sns.heatmap(FP[:,:10].transpose(),  ax=ax, vmin=0, vmax=1,cbar=cbar, label='normal', cbar_kws={'label': '$\\Delta$F'}) \n",
    "    cbar = ax.collections[0].colorbar \n",
    "    cbar.ax.tick_params(labelsize=30) \n",
    "    ax.figure.axes[-1].yaxis.label.set_size(50) \n",
    " \n",
    "  else: \n",
    "    sns.heatmap(FP[:,:10].transpose(),  ax=ax, vmin=0, vmax=1,cbar=False, label='small') \n",
    " \n",
    "  ax.invert_yaxis()\n",
    "  ax.set_title(title, fontsize = 70, pad= 60) \n",
    "  ax.set_xlabel('epoch', fontsize = 60)  \n",
    "  ax.set_ylabel('frequency index', fontsize = 60) \n",
    "  ax.tick_params(labelsize = 30) \n",
    " \n",
    "  xticks=ax.xaxis.get_major_ticks() \n",
    "  for i in range(len(xticks)): \n",
    "      if i%5==0: \n",
    "        continue \n",
    "      else: \n",
    "          xticks[i].set_visible(False) \n",
    " \n",
    " \n",
    "import matplotlib.pyplot as plt \n",
    "# from google.colab import files \n",
    " \n",
    " \n",
    " \n",
    "def plot_barplots(models_name, models_frequencies, epochs, savepath='./HEATMAPS.svg',): \n",
    " \n",
    "  ''' \n",
    "  models_name: a list with the models titles i.e \"[(a) Adam ...,] \" \n",
    "  moels_frequencies: a list with numpy arrays storing the training frequencies per epochs for each model (i.e Adam... ) \n",
    "  epochs: a list with numpy arrays storing the training epochs for each model (i.e Adam... ) \n",
    "  ''' \n",
    " \n",
    "  fig, axs = plt.subplots(1, 4,  figsize=(100,20)) \n",
    " \n",
    "  for k in range(len(models_name)): \n",
    " \n",
    "    name = models_name[k] \n",
    "    FP = models_frequencies[k] \n",
    "    epoch_tr = epochs[k] \n",
    " \n",
    "    plot_Fmap(FP, epoch_tr, axs[k],  cbar=(k==3), title=name) \n",
    "    plt.subplots_adjust(wspace=0.08, hspace=0.00) \n",
    "    plt.grid('on') \n",
    " \n",
    "  plt.savefig(savepath, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "\n",
    "def plot_corrolation(x, y, title):\n",
    "  #create scatterplot\n",
    "  plt.scatter(x, y, label=title)\n",
    "\n",
    "  #calculate equation for trendline\n",
    "  z = np.polyfit(x, y, 1)\n",
    "  p = np.poly1d(z)\n",
    "\n",
    "  #add trendline to plot\n",
    "  plt.plot(x, p(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18WxPSnclZKI"
   },
   "outputs": [],
   "source": [
    "# here we load the data and plot them (for the average of different seeds)\n",
    "scrn=np.load(\"./SCRNdata.npy\",allow_pickle=True)\n",
    "adahess=np.load(\"./AdaHessdata.npy\",allow_pickle=True)\n",
    "adam=np.load(\"./Adamdata.npy\",allow_pickle=True)\n",
    "sgd=np.load(\"./SGDdata.npy\",allow_pickle=True)\n",
    "model_names=[\"SGD\",\"Adam\",\"AdaHessian\",\"SCRN\"]\n",
    "plot_barplots(model_names,np.array([sgd[3].tolist(),adam[3].tolist(),adahess[3].tolist(),scrn[3].tolist()]),[i+1 for i in range (50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uInrXoVYusMj"
   },
   "outputs": [],
   "source": [
    "## plot corrolation map\n",
    "plot_corrolation(np.array(sgd[3].tolist()).mean(axis=1),sgd[5].astype(float),model_names[0])\n",
    "plot_corrolation(np.array(adam[3].tolist()).mean(axis=1),adam[5].astype(float),model_names[1])\n",
    "plot_corrolation(np.array(adahess[3].tolist()).mean(axis=1),adahess[5].astype(float),model_names[2])\n",
    "plot_corrolation(np.array(scrn[3].tolist()).mean(axis=1),scrn[5].astype(float),model_names[3])\n",
    "plt.legend()\n",
    "# plt.title(\"Validation Corrolation\")\n",
    "plt.xlabel('average frequency error',)  \n",
    "plt.ylabel('validation accuracy',) \n",
    "plt.savefig('./val_corr.svg', bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "optimization-mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
